{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1F5lTDp5UPf0",
   "metadata": {
    "id": "1F5lTDp5UPf0"
   },
   "source": [
    "### 1) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
   "metadata": {
    "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6"
   },
   "outputs": [],
   "source": [
    "#poetry add langchain_community chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55152049-e9e5-4952-8e19-409f58cf3ac9",
   "metadata": {
    "id": "55152049-e9e5-4952-8e19-409f58cf3ac9"
   },
   "source": [
    "### 2) OpenAI ì¸ì¦í‚¤ ì„¤ì •\n",
    "https://openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76f68a8-4745-4377-8057-6090b87377d1",
   "metadata": {
    "id": "b76f68a8-4745-4377-8057-6090b87377d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9abc5",
   "metadata": {},
   "source": [
    "##### Chroma ê°„ë‹¨í•œ ì˜ˆì œ\n",
    "* Chroma DBì— í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings  # OpenAI ì„ë² ë”© ì‚¬ìš©\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "from langchain_chroma import Chroma  # ë²¡í„° DB (Chroma) ì‚¬ìš©\n",
    "\n",
    "\n",
    "# 2. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "DB_PATH = \"./db/chroma_db\"\n",
    "\n",
    "# 3. í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def load_and_split_text(file_path, splitter):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•œ í›„, ì„¤ì •ëœ Splitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ\n",
    "        splitter (RecursiveCharacterTextSplitter): í…ìŠ¤íŠ¸ ë¶„í• ê¸° ê°ì²´\n",
    "\n",
    "    Returns:\n",
    "        list: ë¶„í• ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        loader = TextLoader(file_path)  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ\n",
    "        return loader.load_and_split(splitter)  # ë¶„í• í•˜ì—¬ ë°˜í™˜\n",
    "    except Exception as e:\n",
    "        print(f\" íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# 4. í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (600ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , 100ì ê²¹ì¹¨ í¬í•¨)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "\n",
    "# 5. ë‘ ê°œì˜ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ë° ë¶„í• \n",
    "split_doc1 = load_and_split_text(\"data/ai-terminology.txt\", text_splitter)\n",
    "split_doc2 = load_and_split_text(\"data/finance-terminology.txt\", text_splitter)\n",
    "\n",
    "# 6. ë¬¸ì„œ ê°œìˆ˜ ì¶œë ¥\n",
    "print(f\"AI ë¬¸ì„œ ê°œìˆ˜: {len(split_doc1)}\")\n",
    "print(f\"ê¸ˆìœµ ë¬¸ì„œ ê°œìˆ˜: {len(split_doc2)}\")\n",
    "\n",
    "# 7. ëª¨ë“  ë¬¸ì„œ í•©ì¹˜ê¸°\n",
    "all_documents = split_doc1 + split_doc2\n",
    "\n",
    "# 8. Chroma ë²¡í„° DB ìƒì„± ë° ì €ì¥\n",
    "try:\n",
    "    persist_db = Chroma.from_documents(\n",
    "        documents=all_documents,\n",
    "        embedding=OpenAIEmbeddings(),  # OpenAI Embeddings ì‚¬ìš©\n",
    "        persist_directory=DB_PATH,  # ë²¡í„° DB ì €ì¥ ìœ„ì¹˜ ì§€ì •\n",
    "        collection_name=\"my_vector_db\",  # ë°ì´í„°ë² ì´ìŠ¤ ì»¬ë ‰ì…˜ ì´ë¦„\n",
    "    )\n",
    "    print(\"Chroma ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\" Chroma ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 9. ì €ì¥ëœ ë°ì´í„° í™•ì¸\n",
    "try:\n",
    "    retrieved_docs = persist_db.get()  # Chroma DBì—ì„œ ë°ì´í„° ì¡°íšŒ\n",
    "    print(f\" ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: {len(retrieved_docs['ids'])}, íƒ€ì… {type(retrieved_docs['ids'])}\")\n",
    "except Exception as e:\n",
    "    print(f\" ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 10. ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜\n",
    "def search_query(query, k=2):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì ì…ë ¥(query)ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰í•  ë¬¸ì¥ (ì˜ˆ: \"Transformer ê°œë… ì„¤ëª…\")\n",
    "        k (int, optional): ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        None: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = persist_db.similarity_search(query, k=k)  # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        print(f\"\\n [Query]: {query}\\n\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"ğŸ”¹ [Result {i+1}]: {doc.page_content[:300]}...\\n\")  # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "    except Exception as e:\n",
    "        print(f\" ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 11. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "search_query(\"Transformer ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\", k=2)\n",
    "search_query(\"Hedge Fund ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜?\", k=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fbd63b",
   "metadata": {},
   "source": [
    "##### FAISS ê°„ë‹¨í•œ ì˜ˆì œ\n",
    "* FAISS DBì— í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218ba88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI ë¬¸ì„œ ê°œìˆ˜: 6\n",
      "ê¸ˆìœµ ë¬¸ì„œ ê°œìˆ˜: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "from langchain_community.vectorstores import FAISS  # ë²¡í„° DB (FAISS) ì‚¬ìš©\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# 2. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "DB_PATH = \"./faiss_db\"\n",
    "\n",
    "# 3. í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def load_and_split_text(file_path, splitter):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•œ í›„, ì„¤ì •ëœ Splitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ\n",
    "        splitter (RecursiveCharacterTextSplitter): í…ìŠ¤íŠ¸ ë¶„í• ê¸° ê°ì²´\n",
    "\n",
    "    Returns:\n",
    "        list: ë¶„í• ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ\n",
    "        return loader.load_and_split(splitter)  # ë¶„í• í•˜ì—¬ ë°˜í™˜\n",
    "    except Exception as e:\n",
    "        print(f\" íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# 4. í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (600ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , 100ì ê²¹ì¹¨ í¬í•¨)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "\n",
    "# 5. ë‘ ê°œì˜ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ë° ë¶„í• \n",
    "split_doc1 = load_and_split_text(\"../data/ai-terminology.txt\", text_splitter)\n",
    "split_doc2 = load_and_split_text(\"../data/finance-terminology.txt\", text_splitter)\n",
    "\n",
    "# 6. ë¬¸ì„œ ê°œìˆ˜ ì¶œë ¥\n",
    "print(f\"AI ë¬¸ì„œ ê°œìˆ˜: {len(split_doc1)}\")\n",
    "print(f\"ê¸ˆìœµ ë¬¸ì„œ ê°œìˆ˜: {len(split_doc2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c371fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!\n",
      " ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: 11\n",
      " ë²¡í„° ì°¨ì›: 1024\n",
      " ì¸ë±ìŠ¤ íƒ€ì…: <class 'faiss.swigfaiss_avx2.IndexFlatL2'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. ëª¨ë“  ë¬¸ì„œ í•©ì¹˜ê¸°\n",
    "all_documents = split_doc1 + split_doc2\n",
    "\n",
    "# 8. FAISS ë²¡í„° DB ìƒì„± ë° ì €ì¥\n",
    "try:\n",
    "    ollamaEmbeddings = OllamaEmbeddings(model=\"bge-m3:latest\")\n",
    "\n",
    "    # FAISS ë²¡í„° DB ìƒì„±\n",
    "    persist_db = FAISS.from_documents(\n",
    "        documents=all_documents,\n",
    "        embedding=ollamaEmbeddings,\n",
    "    )\n",
    "    \n",
    "    # ë¡œì»¬ ë””ìŠ¤í¬ì— ì €ì¥\n",
    "    if not os.path.exists(DB_PATH):\n",
    "        os.makedirs(DB_PATH)\n",
    "    persist_db.save_local(DB_PATH)\n",
    "    \n",
    "    print(\"FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\" FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 9. ì €ì¥ëœ ë°ì´í„° í™•ì¸\n",
    "try:\n",
    "    # FAISSì—ì„œ ì¸ë±ìŠ¤ ì •ë³´ í™•ì¸\n",
    "    print(f\" ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: {persist_db.index.ntotal}\")\n",
    "    print(f\" ë²¡í„° ì°¨ì›: {persist_db.index.d}\")\n",
    "    print(f\" ì¸ë±ìŠ¤ íƒ€ì…: {type(persist_db.index)}\")\n",
    "except Exception as e:\n",
    "    print(f\" ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10. ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜\n",
    "def search_query(query, k=5):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì ì…ë ¥(query)ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰í•  ë¬¸ì¥ (ì˜ˆ: \"Transformer ê°œë… ì„¤ëª…\")\n",
    "        k (int, optional): ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        None: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = persist_db.similarity_search(query, k=k)  # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        print(f\"\\n [Query]: {query}\\n\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"[Result {i+1}]: {doc.page_content[:300]}...\\n\")  # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "    except Exception as e:\n",
    "        print(f\" ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 11. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print('AI ìš©ì–´ =================')\n",
    "search_query(\"Embedding ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\", k=3)\n",
    "print('Finance ìš©ì–´ =================')\n",
    "search_query(\"Hedge Fund ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\", k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4585881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ì €ì¥ëœ FAISS DB ë¡œë“œ í…ŒìŠ¤íŠ¸\n",
      "==================================================\n",
      "FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!\n",
      "ë¡œë“œëœ ë²¡í„° ê°œìˆ˜: 11\n",
      "\n",
      "ë¡œë“œëœ DB ê²€ìƒ‰ ê²°ê³¼: Zero-shot Learning (ì œë¡œìƒ· í•™ìŠµ)\n",
      "\n",
      "ì •ì˜: íŠ¹ì • íƒœìŠ¤í¬ì— ëŒ€í•œ ë°ì´í„° ì—†ì´ë„ ê¸°ì¡´ í•™ìŠµëœ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì˜ˆì¸¡í•˜ëŠ” AI ê¸°ìˆ .\n",
      "ì˜ˆì‹œ: AIê°€ ìƒˆë¡œìš´ ì–¸ì–´ë¥¼ í•™ìŠµí•˜ì§€ ì•Šì•˜ì§€ë§Œ, ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë²ˆì—­ ê°€ëŠ¥.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ì „ì´ í•™ìŠµ, ì¼ë°˜í™” ëŠ¥ë ¥, NLP\n",
      "\n",
      "Few-shot Learning (í“¨ìƒ· í•™ìŠµ)\n",
      "\n",
      "ì •ì˜: ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œë„ ìƒˆë¡œìš´ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í•™ìŠµ ë°©ë²•.\n",
      "ì˜ˆì‹œ: ChatGPTê°€ ëª‡ ê°œì˜ ì˜ˆì œë§Œìœ¼ë¡œë„ ìƒˆë¡œìš´ ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µì„ ìƒì„±í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ìƒ˜í”Œ íš¨ìœ¨ì„±, ì „ì´ í•™ìŠµ, ë©”íƒ€ í•™ìŠµ\n",
      "\n",
      "Rei...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 12. ì €ì¥ëœ FAISS DB ë¡œë“œ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ì €ì¥ëœ FAISS DB ë¡œë“œ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    ollamaEmbeddings = OllamaEmbeddings(model=\"bge-m3:latest\")\n",
    "    # ì €ì¥ëœ FAISS DB ë¡œë“œ\n",
    "    loaded_db = FAISS.load_local(\n",
    "        DB_PATH, \n",
    "        ollamaEmbeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    print(\"FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ë¡œë“œëœ DBë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "    print(f\"ë¡œë“œëœ ë²¡í„° ê°œìˆ˜: {loaded_db.index.ntotal}\")\n",
    "    \n",
    "    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "    test_results = loaded_db.similarity_search(\"í•™ìŠµì—ëŠ” ì–´ë–¤ê²ƒë“¤ì´ ìˆë‚˜ìš”?\", k=3)\n",
    "    print(f\"\\në¡œë“œëœ DB ê²€ìƒ‰ ê²°ê³¼: {test_results[0].page_content[:300]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"FAISS DB ë¡œë“œ ì˜¤ë¥˜: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78988a",
   "metadata": {},
   "source": [
    "* [k-ìµœê·¼ì ‘ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜](https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)\n",
    "* FAISSì—ì„œ similarity_search_with_score() í•¨ìˆ˜ì˜ ì ìˆ˜ëŠ” ì‹¤ì œë¡œëŠ” **ê±°ë¦¬(distance)**ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "    * ë‚®ì€ ì ìˆ˜ = ë” ìœ ì‚¬í•¨ (ê±°ë¦¬ê°€ ê°€ê¹Œì›€)\n",
    "    : Result 1 (Score: 0.3795) â†’ ë” ìœ ì‚¬í•œ ê²°ê³¼\n",
    "    * ë†’ì€ ì ìˆ˜ = ëœ ìœ ì‚¬í•¨ (ê±°ë¦¬ê°€ ë©€ìŒ)\n",
    "    : Result 2 (Score: 0.4341) â†’ ëœ ìœ ì‚¬í•œ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13183274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰\n",
      "==================================================\n",
      "\n",
      " [Query]: í•™ìŠµì—ëŠ” ì–´ë–¤ê²ƒë“¤ì´ ìˆë‚˜ìš”?\n",
      "\n",
      "[Result 1] (Score: 0.8634):\n",
      "Zero-shot Learning (ì œë¡œìƒ· í•™ìŠµ)\n",
      "\n",
      "ì •ì˜: íŠ¹ì • íƒœìŠ¤í¬ì— ëŒ€í•œ ë°ì´í„° ì—†ì´ë„ ê¸°ì¡´ í•™ìŠµëœ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì˜ˆì¸¡í•˜ëŠ” AI ê¸°ìˆ .\n",
      "ì˜ˆì‹œ: AIê°€ ìƒˆë¡œìš´ ì–¸ì–´ë¥¼ í•™ìŠµí•˜ì§€ ì•Šì•˜ì§€ë§Œ, ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë²ˆì—­ ê°€ëŠ¥.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ì „ì´ í•™ìŠµ, ì¼ë°˜í™” ëŠ¥ë ¥, NLP\n",
      "\n",
      "Few-shot Learning (í“¨ìƒ· í•™ìŠµ)\n",
      "\n",
      "ì •ì˜: ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œë„ ìƒˆë¡œìš´ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í•™ìŠµ ë°©ë²•.\n",
      "ì˜ˆì‹œ: ChatGPTê°€ ëª‡ ê°œì˜ ì˜ˆì œë§Œìœ¼ë¡œë„ ìƒˆë¡œìš´ ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µì„ ìƒì„±í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ìƒ˜í”Œ íš¨ìœ¨ì„±, ì „ì´ í•™ìŠµ, ë©”íƒ€ í•™ìŠµ\n",
      "\n",
      "Reinforcement Learning (ê°•í™” í•™ìŠµ)\n",
      "\n",
      "ì •ì˜: ë³´ìƒì„ ê·¹ëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í–‰ë™ì„ í•™ìŠµí•˜ëŠ” AI ê¸°ë²•.\n",
      "ì˜ˆì‹œ: ì•ŒíŒŒê³ ê°€ ë°”ë‘‘ì—ì„œ ìµœì ì˜ ìˆ˜ë¥¼ ì°¾ê¸° ìœ„í•´ ê°•í™” í•™ìŠµì„ í™œìš©.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ë³´ìƒ ì‹œìŠ¤í…œ, ì •ì±… í•™ìŠµ, ê²Œì„ AI\n",
      "\n",
      "Prompt Engineering (í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§)\n",
      "\n",
      "ì •ì˜: AI ëª¨ë¸ì´ ì›í•˜ëŠ” ì¶œë ¥ì„ ìƒì„±í•˜ë„ë¡ ì…ë ¥ì„ ìµœì í™”í•˜ëŠ” ...\n",
      "\n",
      "[Result 2] (Score: 0.9780):\n",
      "Large Language Model (ëŒ€í˜• ì–¸ì–´ ëª¨ë¸, LLM)\n",
      "\n",
      "ì •ì˜: ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµí•œ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸.\n",
      "ì˜ˆì‹œ: GPT-4, PaLM ë“±ì´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì— í•´ë‹¹.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ìì—°ì–´ ì²˜ë¦¬, íŠ¸ëœìŠ¤í¬ë¨¸, AI ëª¨ë¸\n",
      "\n",
      "Vector Database (ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤)\n",
      "\n",
      "ì •ì˜: ë°ì´í„°ë¥¼ ë²¡í„° í˜•íƒœë¡œ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤.\n",
      "ì˜ˆì‹œ: ChromaDBë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ì˜ë¯¸ë¡ ì  ê²€ìƒ‰, NLP, ì„ë² ë”©\n",
      "\n",
      "Latent Space (ì ì¬ ê³µê°„)\n",
      "\n",
      "ì •ì˜: ë°ì´í„°ê°€ ì €ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜ëœ ê³µê°„ìœ¼ë¡œ, ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë°ì´í„°ë“¤ì´ ê°€ê¹Œì´ ìœ„ì¹˜í•¨.\n",
      "ì˜ˆì‹œ: \"ê°•ì•„ì§€\"ì™€ \"ê³ ì–‘ì´\"ê°€ ê°™ì€ í´ëŸ¬ìŠ¤í„°ì— ë°°ì¹˜ë¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ì°¨ì› ì¶•ì†Œ, ì„ë² ë”©, ë¨¸ì‹ ëŸ¬ë‹\n",
      "\n",
      "Generative AI (ìƒì„±í˜• AI)\n",
      "\n",
      "ì •ì˜: ìƒˆë¡œìš´ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ìŒì•… ë“±ì„ ìƒì„±í•˜ëŠ” AI ê¸°ìˆ .\n",
      "ì˜ˆì‹œ: DALLÂ·Eê°€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±.\n",
      "ì—°ê´€ í‚¤ì›Œ...\n",
      "\n",
      "[Result 3] (Score: 0.9970):\n",
      "Token (í† í°)\n",
      "\n",
      "ì •ì˜: í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë” ì‘ì€ ë‹¨ìœ„(ë‹¨ì–´, ë¬¸ì, ë¬¸ì¥ ë“±)ë¡œ ë‚˜ëˆ„ëŠ” ê³¼ì •.\n",
      "ì˜ˆì‹œ: \"AIëŠ” í˜ì‹ ì ì´ë‹¤\"ë¥¼ [\"AI\", \"ëŠ”\", \"í˜ì‹ ì \", \"ì´ë‹¤\"]ë¡œ ë¶„í• .\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: í† í°í™”, NLP, í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
      "\n",
      "Transformer (íŠ¸ëœìŠ¤í¬ë¨¸)\n",
      "\n",
      "ì •ì˜: ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ë¡œ, ë³‘ë ¬ ì—°ì‚°ê³¼ ì¥ê¸° ì˜ì¡´ì„± ì²˜ë¦¬ê°€ ê°•ì .\n",
      "ì˜ˆì‹œ: GPT, BERT ë“±ì˜ ëª¨ë¸ì´ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ë”¥ëŸ¬ë‹, ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜, NLP\n",
      "\n",
      "Self-Attention (ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜)\n",
      "\n",
      "ì •ì˜: ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ê°€ ì„œë¡œì—ê²Œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ë¬¸ë§¥ì„ ì´í•´í•˜ëŠ” ë°©ì‹.\n",
      "ì˜ˆì‹œ: \"ë‚˜ëŠ” ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•œë‹¤\"ì—ì„œ \"ë‚˜ëŠ”\"ê³¼ \"ì¢‹ì•„í•œë‹¤\"ê°€ ê°•í•œ ì—°ê´€ì„±ì„ ê°€ì§.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: íŠ¸ëœìŠ¤í¬ë¨¸, BERT, ë¬¸ë§¥ í•™ìŠµ\n",
      "\n",
      "Fine-Tuning (ë¯¸ì„¸ ì¡°ì •)\n",
      "\n",
      "ì •ì˜: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ íŠ¹ì • ì‘ì—…ì— ë§ê²Œ ì¶”ê°€ í•™ìŠµí•˜ëŠ” ê³¼ì •.\n",
      "ì˜ˆì‹œ: GPT ëª¨ë¸ì„ ë²•ë¥  ë¬¸ì„œ ìš”ì•½ì— ë§ê²Œ í•™...\n",
      "\n",
      "[Result 4] (Score: 1.0518):\n",
      "Diffusion Model (í™•ì‚° ëª¨ë¸)\n",
      "\n",
      "ì •ì˜: ì´ë¯¸ì§€ ìƒì„± AIì—ì„œ ë…¸ì´ì¦ˆë¥¼ ì ì§„ì ìœ¼ë¡œ ì œê±°í•˜ë©° ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ë²•.\n",
      "ì˜ˆì‹œ: Stable Diffusionì´ í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ì´ë¯¸ì§€ ìƒì„±, ë”¥ëŸ¬ë‹, ìƒì„± AI\n",
      "\n",
      "Hallucination (í™˜ê°)\n",
      "\n",
      "ì •ì˜: AIê°€ ì‹¤ì œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì •ë³´ë‚˜ ì˜ëª»ëœ ë‚´ìš©ì„ ìƒì„±í•˜ëŠ” í˜„ìƒ.\n",
      "ì˜ˆì‹œ: AIê°€ ê°€ì§œ ë…¼ë¬¸ì„ ë§Œë“¤ì–´ë‚´ëŠ” ê²½ìš°.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ì‹ ë¢°ì„±, ëª¨ë¸ í‰ê°€, ìì—°ì–´ ì²˜ë¦¬\n",
      "\n",
      "RAG (Retrieval-Augmented Generation, ê²€ìƒ‰ ê¸°ë°˜ ìƒì„±)\n",
      "\n",
      "ì •ì˜: LLMì´ ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì‘ë‹µì˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ê¸°ë²•.\n",
      "ì˜ˆì‹œ: LangChainì„ í™œìš©í•œ ë¬¸ì„œ ê²€ìƒ‰ ê¸°ë°˜ AI ì±—ë´‡.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ì§€ì‹ ê²€ìƒ‰, í•˜ì´ë¸Œë¦¬ë“œ AI, LangChain\n",
      "\n",
      "Multimodal AI (ë©€í‹°ëª¨ë‹¬ AI)\n",
      "\n",
      "ì •ì˜: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ì—¬ëŸ¬ ìœ í˜•ì˜ ë°ì´í„°ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” AI ê¸°ìˆ .\n",
      "ì˜ˆì‹œ: GPT...\n",
      "\n",
      "[Result 5] (Score: 1.1032):\n",
      "Explainable AI (ì„¤ëª… ê°€ëŠ¥í•œ AI, XAI)\n",
      "\n",
      "ì •ì˜: AIì˜ ì˜ì‚¬ê²°ì • ê³¼ì •ì´ ì‚¬ëŒì—ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…ë˜ëŠ” ê¸°ìˆ .\n",
      "ì˜ˆì‹œ: AIê°€ ì˜ë£Œ ì§„ë‹¨ì„ ë‚´ë¦´ ë•Œ, ì™œ ê·¸ëŸ° ê²°ë¡ ì„ ë‚´ë ¸ëŠ”ì§€ ì„¤ëª…í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ì‹ ë¢°ì„±, ëª¨ë¸ í•´ì„, AI ìœ¤ë¦¬\n",
      "\n",
      "AGI (Artificial General Intelligence, ë²”ìš© ì¸ê³µì§€ëŠ¥)\n",
      "\n",
      "ì •ì˜: íŠ¹ì • ì‘ì—…ì´ ì•„ë‹ˆë¼ ì¸ê°„ì²˜ëŸ¼ ë‹¤ì–‘í•œ ì§€ì  ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” AI ê°œë….\n",
      "ì˜ˆì‹œ: í˜„ì¬ AIëŠ” íŠ¹ì • ì‘ì—…ì— íŠ¹í™”ëœ ANI(íŠ¹ì • ì¸ê³µì§€ëŠ¥)ì´ì§€ë§Œ, ë¯¸ë˜ì—ëŠ” AGI ê°œë°œì´ ëª©í‘œ.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ììœ¨ í•™ìŠµ, ê°•í•œ AI, ë¯¸ë˜ ê¸°ìˆ ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 13. ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def search_with_score(query, k=5):\n",
    "    \"\"\"\n",
    "    ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = persist_db.similarity_search_with_score(query, k=k)\n",
    "        print(f\"\\n [Query]: {query}\\n\")\n",
    "        for i, (doc, score) in enumerate(results):\n",
    "            print(f\"[Result {i+1}] (Score: {score:.4f}):\")\n",
    "            print(f\"{doc.page_content[:500]}...\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\" ì ìˆ˜ ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "search_with_score(\"í•™ìŠµì—ëŠ” ì–´ë–¤ê²ƒë“¤ì´ ìˆë‚˜ìš”?\", k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4fa42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 14. FAISS ì¸ë±ìŠ¤ ì •ë³´ ì¶œë ¥\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FAISS ì¸ë±ìŠ¤ ìƒì„¸ ì •ë³´\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    index = persist_db.index\n",
    "    print(f\"ì¸ë±ìŠ¤ íƒ€ì…: {type(index).__name__}\")\n",
    "    print(f\"ì´ ë²¡í„° ìˆ˜: {index.ntotal}\")\n",
    "    print(f\"ë²¡í„° ì°¨ì›: {index.d}\")\n",
    "    print(f\"í›ˆë ¨ ì—¬ë¶€: {index.is_trained}\")\n",
    "    print(f\"ë©”íŠ¸ë¦­ íƒ€ì…: {index.metric_type}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì¶”ì •)\n",
    "    memory_usage = index.ntotal * index.d * 4 / (1024 * 1024)  # 4 bytes per float, MB ë‹¨ìœ„\n",
    "    print(f\"ì¶”ì • ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d910ef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
