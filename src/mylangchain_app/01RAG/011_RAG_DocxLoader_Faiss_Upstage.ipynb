{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry add docx2txt\n",
    "# poetry add langchain-upstage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WA\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv()\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "print(UPSTAGE_API_KEY[30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Knowledge Base 구성을 위한 데이터 생성\n",
    "\n",
    "- [RecursiveCharacterTextSplitter](https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/)를 활용한 데이터 chunking\n",
    "    - split 된 데이터 chunk를 Large Language Model(LLM)에게 전달하면 토큰 절약 가능\n",
    "    - 비용 감소와 답변 생성시간 감소의 효과\n",
    "    - LangChain에서 다양한 [TextSplitter](https://python.langchain.com/v0.2/docs/how_to/#text-splitters)들을 제공\n",
    "- `chunk_size` 는 split 된 chunk의 최대 크기\n",
    "- `chunk_overlap`은 앞 뒤로 나뉘어진 chunk들이 얼마나 겹쳐도 되는지 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n",
      "<class 'langchain_core.documents.base.Document'>\n",
      "[Document(metadata={'source': '../data/tax_with_table.docx'}, page_content='소득세법\\n\\n소득세법\\n\\n[시행 2024. 1. 1.] [법률 제19933호, 2023. 12. 31., 일부개정]\\n\\n기획재정부(소득세제과(사업소득, 기타소득)) 044-215-4217\\n\\n기획재정부(소득세제과(근로소득)) 044-215-4216\\n\\n기획재정부(재산세제과(양도소득세)) 044-215-4314\\n\\n기획재정부(금융세제과(이자소득, 배당소득)) 044-215-4236\\n\\n\\n\\n\\t제1장 총칙 <개정 2009. 12. 31.>\\t\\n\\n\\n\\n제1조(목적) 이 법은 개인의 소득에 대하여 소득의 성격과 납세자의 부담능력 등에 따라 적정하게 과세함으로써 조세부담의 형평을 도모하고 재정수입의 원활한 조달에 이바지함을 목적으로 한다.\\n\\n[본조신설 2009. 12. 31.]\\n\\n[종전 제1조는 제2조로 이동 <2009. 12. 31.>]\\n\\n\\n\\n제1조의2(정의) ① 이 법에서 사용하는 용어의 뜻은 다음과 같다. <개정 2010. 12. 27., 2014. 12. 23., 2018. 12. 31.>\\n\\n1. “거주자”란 국내에 주소를 두거나 183일 이상의 거소(居所)를 둔 개인을 말한다.\\n\\n2. “비거주자”란 거주자가 아닌 개인을 말한다.\\n\\n3. “내국법인”이란 「법인세법」 제2조제1호에 따른 내국법인을 말한다.\\n\\n4. “외국법인”이란 「법인세법」 제2조제3호에 따른 외국법인을 말한다.\\n\\n5. “사업자”란 사업소득이 있는 거주자를 말한다.\\n\\n② 제1항에 따른 주소ㆍ거소와 거주자ㆍ비거주자의 구분은 대통령령으로 정한다.\\n\\n[본조신설 2009. 12. 31.]\\n\\n\\n\\n제2조(납세의무) ① 다음 각 호의 어느 하나에 해당하는 개인은 이 법에 따라 각자의 소득에 대한 소득세를 납부할 의무를 진다.\\n\\n1. 거주자\\n\\n2. 비거주자로서 국내원천소득(國內源泉所得)이 있는 개인\\n\\n② 다음 각 호의 어느 하나에 해당하는 자는 이 법에 따라 원천징수한 소득세를 납부할 의무를 진다.\\n\\n1. 거주자\\n\\n2. 비거주자\\n\\n3. 내국법인\\n\\n4. 외국법인의 국내지점 또는 국내영업소(출장소, 그 밖에 이에 준하는 것을 포함한다. 이하 같다)\\n\\n5. 그 밖에 이 법에서 정하는 원천징수의무자\\n\\n③ 「국세기본법」 제13조제1항에 따른 법인 아닌 단체 중 같은 조 제4항에 따른 법인으로 보는 단체(이하 “법인으로 보는 단체”라 한다) 외의 법인 아닌 단체는 국내에 주사무소 또는 사업의 실질적 관리장소를 둔 경우에는 1거주자로, 그 밖의 경우에는 1비거주자로 보아 이 법을 적용한다. 다만, 다음 각 호의 어느 하나에 해당하는 경우에는 소득구분에 따라 해당 단체의 각 구성원별로 이 법 또는 「법인세법」에 따라 소득에 대한 소득세 또는 법인세[해당 구성원이 「법인세법」에 따른 법인(법인으로 보는 단체를 포함한다)인 경우로 한정한다. 이하 이 조에서 같다]를 납부할 의무를 진다.<개정 2010. 12. 27., 2013. 1. 1., 2018. 12. 31.>\\n\\n1. 구성원 간 이익의 분배비율이 정하여져 있고 해당 구성원별로 이익의 분배비율이 확인되는 경우'), Document(metadata={'source': '../data/tax_with_table.docx'}, page_content='1. 구성원 간 이익의 분배비율이 정하여져 있고 해당 구성원별로 이익의 분배비율이 확인되는 경우\\n\\n2. 구성원 간 이익의 분배비율이 정하여져 있지 아니하나 사실상 구성원별로 이익이 분배되는 것으로 확인되는 경우\\n\\n④ 제3항에도 불구하고 해당 단체의 전체 구성원 중 일부 구성원의 분배비율만 확인되거나 일부 구성원에게만 이익이 분배되는 것으로 확인되는 경우에는 다음 각 호의 구분에 따라 소득세 또는 법인세를 납부할 의무를 진다.<신설 2018. 12. 31.>\\n\\n1. 확인되는 부분: 해당 구성원별로 소득세 또는 법인세에 대한 납세의무 부담\\n\\n2. 확인되지 아니하는 부분: 해당 단체를 1거주자 또는 1비거주자로 보아 소득세에 대한 납세의무 부담\\n\\n⑤ 제3항 및 제4항에도 불구하고 법인으로 보는 단체 외의 법인 아닌 단체에 해당하는 국외투자기구(투자권유를 하여 모은 금전 등을 가지고 재산적 가치가 있는 투자대상자산을 취득, 처분하거나 그 밖의 방법으로 운용하고 그 결과를 투자자에게 배분하여 귀속시키는 투자행위를 하는 기구로서 국외에서 설립된 기구를 말한다. 이하 같다)를 제119조의2제1항제2호에 따라 국내원천소득의 실질귀속자로 보는 경우 그 국외투자기구는 1비거주자로서 소득세를 납부할 의무를 진다.<신설 2018. 12. 31.>\\n\\n[전문개정 2009. 12. 31.]\\n\\n[제1조에서 이동, 종전 제2조는 제2조의2로 이동 <2009. 12. 31.>]\\n\\n\\n\\n제2조의2(납세의무의 범위) ① 제43조에 따라 공동사업에 관한 소득금액을 계산하는 경우에는 해당 공동사업자별로 납세의무를 진다. 다만, 제43조제3항에 따른 주된 공동사업자(이하 이 항에서 “주된 공동사업자”라 한다)에게 합산과세되는 경우 그 합산과세되는 소득금액에 대해서는 주된 공동사업자의 특수관계인은 같은 조 제2항에 따른 손익분배비율에 해당하는 그의 소득금액을 한도로 주된 공동사업자와 연대하여 납세의무를 진다. <개정 2012. 1. 1., 2013. 1. 1.>\\n\\n② 제44조에 따라 피상속인의 소득금액에 대해서 과세하는 경우에는 그 상속인이 납세의무를 진다.\\n\\n③ 제101조제2항에 따라 증여자가 자산을 직접 양도한 것으로 보는 경우 그 양도소득에 대해서는 증여자와 증여받은 자가 연대하여 납세의무를 진다.<개정 2020. 12. 29.>\\n\\n④ 제127조에 따라 원천징수되는 소득으로서 제14조제3항 또는 다른 법률에 따라 제14조제2항에 따른 종합소득과세표준에 합산되지 아니하는 소득이 있는 자는 그 원천징수되는 소득세에 대해서 납세의무를 진다.<개정 2020. 12. 29.>\\n\\n⑤ 공동으로 소유한 자산에 대한 양도소득금액을 계산하는 경우에는 해당 자산을 공동으로 소유하는 각 거주자가 납세의무를 진다.<신설 2017. 12. 19., 2020. 12. 29.>\\n\\n[전문개정 2009. 12. 31.]\\n\\n[제2조에서 이동 <2009. 12. 31.>]')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "loader = Docx2txtLoader('../data/tax_with_table.docx')\n",
    "document_list = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "print(len(document_list))\n",
    "print(type(document_list[0]))\n",
    "print(document_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.embeddings.Embeddings object at 0x000001DBCC49FE00> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001DBCC4BF830> model='solar-embedding-1-large' dimensions=None upstage_api_key=SecretStr('**********') upstage_api_base='https://api.upstage.ai/v1/solar' embedding_ctx_length=4096 embed_batch_size=10 allowed_special=set() disallowed_special='all' chunk_size=1000 max_retries=2 request_timeout=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers={'x-upstage-client': 'langchain'} default_query=None http_client=None http_async_client=None\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.faiss.FAISS object at 0x000001DBD0F2E900>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 데이터를 처음 저장할 때 \n",
    "database = FAISS.from_documents(documents=document_list, embedding=embeddings)\n",
    "# 로컬 파일로 저장\n",
    "database.save_local(\"faiss_docdb\")\n",
    "\n",
    "# 이미 저장된 데이터를 사용할 때 \n",
    "# database = FAISS.load_local(\"faiss_docdb\", embedding, allow_dangerous_deserialization=True)\n",
    "print(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 답변 생성을 위한 Retrieval\n",
    "\n",
    "- `FAISS`에 저장한 데이터를 유사도 검색(`similarity_search()`)를 활용해서 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "<class 'langchain_core.documents.base.Document'>\n",
      "{'source': '../data/tax_with_table.docx'}\n"
     ]
    }
   ],
   "source": [
    "query = '비과세소득에는 어떤것들이 있나요?'\n",
    "\n",
    "# `k` 값을 조절해서 얼마나 많은 데이터를 불러올지 결정\n",
    "retrieved_docs = database.similarity_search(query, k=6)\n",
    "\n",
    "print(len(retrieved_docs))\n",
    "print(type(retrieved_docs[0]))\n",
    "print(retrieved_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마. 「국군포로의 송환 및 대우 등에 관한 법률」에 따라 국군포로가 받는 위로지원금과 그 밖의 금품\n",
      "\n",
      "바. 「문화재보호법」에 따라 국가지정문화재로 지정된 서화ㆍ골동품의 양도로 발생\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Augmentation을 위한 Prompt 활용\n",
    "\n",
    "- Retrieval된 데이터는 LangChain에서 제공하는 프롬프트(`\"rlm/rag-prompt\"`) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> 6. 생성 → LLM으로 답변 생성\n",
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001DBD0F2C4D0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001DC091558B0> model_name='solar-pro' temperature=0.5 model_kwargs={} upstage_api_key=SecretStr('**********') upstage_api_base='https://api.upstage.ai/v1'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "print(\"===> 6. 생성 → LLM으로 답변 생성\")\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 답변 생성\n",
    "\n",
    "- [RetrievalQA](https://docs.smith.langchain.com/old/cookbook/hub-examples/retrieval-qa-chain)를 통해 LLM에 전달\n",
    "    - `RetrievalQA`는 [create_retrieval_chain](https://python.langchain.com/v0.2/docs/how_to/qa_sources/#using-create_retrieval_chain)으로 대체됨\n",
    "    - 실제 ChatBot 구현 시 `create_retrieval_chain`으로 변경하는 과정을 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from pprint import pprint\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, \n",
    "    retriever=database.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '비과세소득에는 어떤것들이 있나요?',\n",
      " 'result': '비과세소득에는 국군포로의 위로지원금, 국가지정문화재 서화·골동품 양도 소득, 박물관·미술관 양도 소득, 종교인소득 중 '\n",
      "           '학자금·식사대·실비변상적 지급액·월 20만원 이내 보육 지원금·사택 이익, 위원회 수당 등이 포함됩니다. 또한 공익신탁 '\n",
      "           '이익, 논·밭 작물 생산 소득, 1주택 임대소득(12억원 미만), 학자금, 실비변상적 급여 등도 비과세 대상입니다. '\n",
      "           '구체적인 항목은 관련 법령 및 시행령에서 추가로 규정하고 있습니다.'}\n"
     ]
    }
   ],
   "source": [
    "query = '비과세소득에는 어떤것들이 있나요?'\n",
    "\n",
    "ai_message = qa_chain.invoke({\"query\": query})\n",
    "pprint(ai_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '과세소득의 범위 및 소득의 구분에는 어떤것들이 있나요?',\n",
      " 'result': '과세소득의 범위는 거주자의 경우 모든 소득, 비거주자의 경우 국내원천소득에 한정되며(제3조), 소득 구분은 거주자에 '\n",
      "           '대해 종합소득(이자·배당·사업·근로·연금·기타소득), 퇴직소득, 금융투자소득, 양도소득으로 분류됩니다(제4조). '\n",
      "           '신탁소득의 경우 수익자 또는 위탁자에게 귀속됩니다(제2조의3).'}\n"
     ]
    }
   ],
   "source": [
    "query = '과세소득의 범위 및 소득의 구분에는 어떤것들이 있나요?'\n",
    "\n",
    "ai_message = qa_chain.invoke({\"query\": query})\n",
    "pprint(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LangChain 기반의 RAG(Retrieval-Augmented Generation) 파이프라인을 구현하여 DOCX 문서를 로드, 임베딩, 검색, 그리고 LLM을 통한 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 로드 완료\n",
      "문서 분할 완료: 720개 청크 생성\n",
      "벡터 저장소 생성 완료\n",
      "{'query': '총수입금액 불산입에 대하여 설명해 주세요.', 'result': '총수입금액 불산입은 소득세 계산 시 특정 항목을 총수입금액에 포함하지 않는 것을 의미합니다. 이는 개별소비세, 주세, 국세/지방세 환급가산금 이자, 부가가치세 매출세액 등이 해당되며, 일부 무상 자산 가액이나 이월소득금액도 제외됩니다. 단, 원재료 매입 시 부담하는 세액 등은 예외적으로 산입됩니다.'}\n",
      "\n",
      " AI의 답변:\n",
      "총수입금액 불산입은 소득세 계산 시 특정 항목을 총수입금액에 포함하지 않는 것을 의미합니다. 이는 개별소비세, 주세, 국세/지방세 환급가산금 이자, 부가가치세 매출세액 등이 해당되며, 일부 무상 자산 가액이나 이월소득금액도 제외됩니다. 단, 원재료 매입 시 부담하는 세액 등은 예외적으로 산입됩니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # 특정 경고 유형만 무시\n",
    "\n",
    "#  1. 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "#  2. OpenAI API 키 확인\n",
    "OPENAI_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"UPSTAGE API 키가 설정되지 않았습니다. .env 파일을 확인하세요.\")\n",
    "\n",
    "#  3. DOCX 파일 로드 및 텍스트 추출 (Docx2txtLoader 활용)\n",
    "def load_docx(file_path):\n",
    "    \"\"\"DOCX 파일에서 텍스트를 추출하는 함수.\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"문서에서 텍스트를 추출할 수 없습니다.\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"문서 로딩 실패: {str(e)}\")\n",
    "\n",
    "#  4. 문서 분할 함수\n",
    "def split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"텍스트를 지정된 크기의 청크로 분할하는 함수.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "#  5. 벡터 데이터베이스(FAISS) 생성 함수\n",
    "def create_vector_store(text_chunks, embedding_model):\n",
    "    \"\"\"텍스트 청크를 임베딩하고 FAISS 벡터 저장소에 저장.\"\"\"\n",
    "    try:\n",
    "        #List[Document]\n",
    "        documents = [Document(page_content=chunk) for chunk in text_chunks]\n",
    "        vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "        vector_store.save_local(\"faiss_docdb\")\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"벡터 저장소 생성 실패: {str(e)}\")\n",
    "\n",
    "#  6. LLM을 활용한 질문 응답 함수\n",
    "def query_with_llm(query, vector_store):\n",
    "    \"\"\"LLM을 사용하여 검색된 문서 기반으로 답변 생성.\"\"\"\n",
    "    try:\n",
    "        # LLM 모델 설정\n",
    "        llm = ChatUpstage(\n",
    "            model=\"solar-pro\",\n",
    "            base_url=\"https://api.upstage.ai/v1\",\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        # 프롬프트 로드 (RAG 최적화된 LangChain Hub 프롬프트 사용)\n",
    "        prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "        # RetrievalQA 체인 생성\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm, \n",
    "            retriever=vector_store.as_retriever(),\n",
    "            chain_type_kwargs={\"prompt\": prompt}\n",
    "        )\n",
    "\n",
    "        # LLM 응답 생성\n",
    "        ai_message = qa_chain.invoke({\"query\": query})\n",
    "        print(ai_message)\n",
    "        return ai_message[\"result\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM 응답 생성 실패: {str(e)}\")\n",
    "\n",
    "#  실행 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # DOCX 파일 경로\n",
    "    docx_path = \"../data/tax_with_table.docx\"\n",
    "    \n",
    "    # 1. 문서 로드\n",
    "    text = load_docx(docx_path)\n",
    "    print(\"문서 로드 완료\")\n",
    "    \n",
    "    # 2. 문서 분할\n",
    "    text_chunks = split_text(text)\n",
    "    print(f\"문서 분할 완료: {len(text_chunks)}개 청크 생성\")\n",
    "    \n",
    "    # 3. 임베딩 모델 초기화\n",
    "    embedding_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "    \n",
    "    # 4. 벡터 저장소 생성\n",
    "    vector_store = create_vector_store(text_chunks, embedding_model)\n",
    "    print(\"벡터 저장소 생성 완료\")\n",
    "    \n",
    "    # 5. 질의 실행\n",
    "    query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store)\n",
    "    \n",
    "    # 6. AI 응답 출력\n",
    "    print(\"\\n AI의 답변:\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('총수입금액 불산입은 소득세 계산 시 특정 항목을 총수입금액에 포함하지 않는 것을 의미합니다. 이는 개별소비세, 주세, 국세/지방세 '\n",
      " '환급가산금 이자, 부가가치세 매출세액 등이 해당되며, 일부 무상 자산 가액이나 이월소득금액도 제외됩니다. 단, 원재료 매입 시 부담하는 '\n",
      " '세액 등은 예외적으로 산입됩니다.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  개선된 Source Level1\n",
    "```python\n",
    "RuntimeError: 벡터 저장소 생성 실패: Error code: 400 - \n",
    "{'error': {\n",
    "    'message': 'Requested 313741 tokens, max 300000 tokens per request', \n",
    "    'type': 'max_tokens_per_request', \n",
    "    'param': None, 'code': 'max_tokens_per_request'\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 개선된 RAG 파이프라인 실행 ===\n",
      "\n",
      "1. 문서 로드 중...\n",
      "   문서 로드 완료: 289,214 문자\n",
      "\n",
      "2. 문서 분할 중...\n",
      "   문서 분할 완료: 511개 청크 생성\n",
      "\n",
      "3. 임베딩 모델 초기화...\n",
      "   임베딩 모델 초기화 완료\n",
      "\n",
      "4. 벡터 저장소 생성 중...\n",
      "    벡터 저장소 생성 완료\n",
      "\n",
      "5. 질의 실행 중...\n",
      "질의: 총수입금액 불산입에 대하여 설명해 주세요.\n",
      "하이브리드 검색 수행 중...\n",
      "검색된 관련 문서: 7개\n",
      "LLM 응답 생성 중...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "#  1. 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "#  2. OpenAI API 키 확인\n",
    "OPENAI_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"UPSTAGE API 키가 설정되지 않았습니다. .env 파일을 확인하세요.\")\n",
    "\n",
    "# 2. 한국어 법률 문서 전용 텍스트 전처리 함수\n",
    "def preprocess_korean_legal_text(text):\n",
    "    \"\"\"한국어 법률 문서를 위한 전처리.\"\"\"\n",
    "    # 불필요한 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 조항 번호 정규화 (제1조, 제2조 등)\n",
    "    text = re.sub(r'제(\\d+)조', r'제\\1조', text)\n",
    "    \n",
    "    # 항 번호 정규화\n",
    "    text = re.sub(r'①|②|③|④|⑤|⑥|⑦|⑧|⑨|⑩', \n",
    "                  lambda m: f\"제{ord(m.group()) - ord('①') + 1}항\", text)\n",
    "    \n",
    "    # 호 번호 정규화\n",
    "    text = re.sub(r'(\\d+)\\.\\s', r'제\\1호 ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# 3. 개선된 문서 분할 함수\n",
    "def advanced_split_text(text, chunk_size=800, chunk_overlap=200):\n",
    "    \"\"\"법률 문서에 최적화된 텍스트 분할.\"\"\"\n",
    "    # 전처리\n",
    "    text = preprocess_korean_legal_text(text)\n",
    "    \n",
    "    # 법률 문서 구조를 고려한 분할자\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n제\", \"\\n**제\",  # 조항 분할\n",
    "            \"\\n①\", \"\\n②\", \"\\n③\", \"\\n④\", \"\\n⑤\",  # 항 분할\n",
    "            \"\\n1.\", \"\\n2.\", \"\\n3.\", \"\\n4.\", \"\\n5.\",  # 호 분할\n",
    "            \"\\n가.\", \"\\n나.\", \"\\n다.\", \"\\n라.\", \"\\n마.\",  # 목 분할\n",
    "            \"\\n\\n\",  # 문단 분할\n",
    "            \"\\n\",    # 줄 분할\n",
    "            \". \",    # 문장 분할\n",
    "            \" \",     # 단어 분할\n",
    "            \"\"       # 문자 분할\n",
    "        ]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# 4. 개선된 문서 로더\n",
    "def load_docx_advanced(file_path):\n",
    "    \"\"\"개선된 DOCX 파일 로더.\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        \n",
    "        if not text.strip():\n",
    "            raise ValueError(\"문서에서 텍스트를 추출할 수 없습니다.\")\n",
    "        \n",
    "        # 기본 정리\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # 여러 개의 빈 줄을 두 개로 통일\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)  # 여러 공백을 하나로 통일\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"문서 로딩 실패: {str(e)}\")\n",
    "\n",
    "# 5. 벡터 저장소 생성 함수\n",
    "def create_vector_store(text_chunks, embedding_model):\n",
    "    \"\"\"메타데이터가 포함된 벡터 저장소 생성.\"\"\"\n",
    "    try:\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            # 메타데이터 추가\n",
    "            metadata = {\n",
    "                'chunk_id': i,\n",
    "                'chunk_length': len(chunk),\n",
    "                'chunk_type': 'legal_document'\n",
    "            }\n",
    "            \n",
    "            # 조항 정보 추출\n",
    "            if '제' in chunk and '조' in chunk:\n",
    "                article_match = re.search(r'제(\\d+)조', chunk)\n",
    "                if article_match:\n",
    "                    metadata['article'] = f\"제{article_match.group(1)}조\"\n",
    "            \n",
    "            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "        # \n",
    "        vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "        vector_store.save_local(\"faiss_docdb\")\n",
    "        return vector_store, documents\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"벡터 저장소 생성 실패: {str(e)}\")\n",
    "\n",
    "# 6. 키워드 기반 검색 함수\n",
    "def keyword_search(query, documents, k=5):\n",
    "    \"\"\"간단한 키워드 기반 검색.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # 각 문서의 점수 계산\n",
    "    scores = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        content_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 교집합 단어 수로 점수 계산\n",
    "        intersection = query_words.intersection(content_words)\n",
    "        score = len(intersection) / len(query_words) if query_words else 0\n",
    "        \n",
    "        # 정확한 구문 매칭 보너스\n",
    "        if query.lower() in doc.page_content.lower():\n",
    "            score += 0.5\n",
    "        \n",
    "        scores.append((score, i, doc))\n",
    "    \n",
    "    # 점수순 정렬하여 상위 k개 반환\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, _, doc in scores[:k]]\n",
    "\n",
    "# 7. 하이브리드 검색 함수\n",
    "def hybrid_search(query, vector_store, documents, k=5, alpha=0.7):\n",
    "    \"\"\"벡터 검색과 키워드 검색을 결합한 하이브리드 검색.\"\"\"\n",
    "    \n",
    "    # 1. 벡터 유사도 검색\n",
    "    vector_results = vector_store.similarity_search(query, k=k*2)  # 더 많이 가져와서 다양성 확보\n",
    "    \n",
    "    # 2. 키워드 검색\n",
    "    keyword_results = keyword_search(query, documents, k=k*2)\n",
    "    \n",
    "    # 3. 결과 합치기 및 점수 계산\n",
    "    combined_results = {}\n",
    "    \n",
    "    # 벡터 검색 결과 점수 (alpha 가중치)\n",
    "    for i, doc in enumerate(vector_results):\n",
    "        doc_id = doc.page_content\n",
    "        vector_score = alpha * (1.0 - i / len(vector_results))\n",
    "        combined_results[doc_id] = {\n",
    "            'document': doc,\n",
    "            'score': vector_score,\n",
    "            'vector_rank': i + 1\n",
    "        }\n",
    "    \n",
    "    # 키워드 검색 결과 점수 ((1-alpha) 가중치)\n",
    "    for i, doc in enumerate(keyword_results):\n",
    "        doc_id = doc.page_content\n",
    "        keyword_score = (1 - alpha) * (1.0 - i / len(keyword_results))\n",
    "        \n",
    "        if doc_id in combined_results:\n",
    "            # 이미 있는 문서면 점수 합산\n",
    "            combined_results[doc_id]['score'] += keyword_score\n",
    "            combined_results[doc_id]['keyword_rank'] = i + 1\n",
    "        else:\n",
    "            # 새로운 문서면 추가\n",
    "            combined_results[doc_id] = {\n",
    "                'document': doc,\n",
    "                'score': keyword_score,\n",
    "                'keyword_rank': i + 1\n",
    "            }\n",
    "    \n",
    "    # 4. 점수순으로 정렬하여 상위 k개 반환\n",
    "    sorted_results = sorted(combined_results.values(), key=lambda x: x['score'], reverse=True)\n",
    "    return [result['document'] for result in sorted_results[:k]]\n",
    "\n",
    "# 8. 한국어 법률 문서 전용 프롬프트 생성 함수\n",
    "def create_korean_legal_prompt():\n",
    "    \"\"\"한국어 법률 문서용 프롬프트 템플릿.\"\"\"\n",
    "    template = \"\"\"당신은 한국 세법 전문가입니다. 주어진 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해야 합니다.\n",
    "\n",
    "다음 규칙을 반드시 따르세요:\n",
    "1. 법조문의 조항, 항, 호, 목을 정확히 인용하세요\n",
    "2. 전문 용어를 사용할 때는 쉬운 설명을 함께 제공하세요\n",
    "3. 관련 조항들 간의 연관성을 설명하세요\n",
    "4. 실무적 적용 방법도 함께 설명하세요\n",
    "5. 불확실한 내용이 있으면 명시적으로 언급하세요\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "위 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해주세요. 관련 조항을 인용하며 설명해주세요.\"\"\"\n",
    "\n",
    "    return PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "# 9. 질문 응답 함수\n",
    "def query_with_llm(query, vector_store, documents):\n",
    "    \"\"\"개선된 LLM 기반 질문 응답.\"\"\"\n",
    "    try:\n",
    "        # LLM 모델 설정\n",
    "        llm = ChatUpstage(\n",
    "            model=\"solar-pro\",\n",
    "            base_url=\"https://api.upstage.ai/v1\",\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        print(f\"질의: {query}\")\n",
    "        print(\"하이브리드 검색 수행 중...\")\n",
    "        \n",
    "        # 하이브리드 검색으로 관련 문서 찾기\n",
    "        relevant_docs = hybrid_search(query, vector_store, documents, k=7, alpha=0.7)\n",
    "        \n",
    "        print(f\"검색된 관련 문서: {len(relevant_docs)}개\")\n",
    "        \n",
    "        # 컨텍스트 구성\n",
    "        context = \"\\n\\n\".join([f\"[문서 {i+1}]\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # 한국어 법률 문서용 프롬프트\n",
    "        prompt = create_korean_legal_prompt()\n",
    "        \n",
    "        # 프롬프트 생성\n",
    "        formatted_prompt = prompt.format(context=context, question=query)\n",
    "        \n",
    "        print(\"LLM 응답 생성 중...\")\n",
    "        \n",
    "        # LLM 응답 생성\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"source_documents\": relevant_docs,\n",
    "            \"context_used\": context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM 응답 생성 실패: {str(e)}\")\n",
    "\n",
    "# 10. 컨텍스트 품질 평가 함수\n",
    "def evaluate_context_quality(query, retrieved_docs):\n",
    "    \"\"\"검색된 문서의 품질을 간단히 평가.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    quality_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 키워드 매칭 점수\n",
    "        keyword_match = len(query_words.intersection(doc_words)) / len(query_words)\n",
    "        \n",
    "        # 문서 길이 점수 (너무 짧거나 길지 않은 것이 좋음)\n",
    "        length_score = min(len(doc.page_content) / 1000, 1.0)\n",
    "        \n",
    "        # 종합 점수\n",
    "        total_score = (keyword_match * 0.7) + (length_score * 0.3)\n",
    "        quality_scores.append(total_score)\n",
    "    \n",
    "    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n",
    "    return avg_quality\n",
    "\n",
    "# 실행 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # DOCX 파일 경로\n",
    "    docx_path = \"../data/tax_with_table.docx\"\n",
    "    \n",
    "    print(\"=== 개선된 RAG 파이프라인 실행 ===\\n\")\n",
    "    \n",
    "    # 1. 문서 로드\n",
    "    print(\"1. 문서 로드 중...\")\n",
    "    text = load_docx_advanced(docx_path)\n",
    "    print(f\"   문서 로드 완료: {len(text):,} 문자\\n\")\n",
    "    \n",
    "    # 2. 개선된 문서 분할\n",
    "    print(\"2. 문서 분할 중...\")\n",
    "    text_chunks = advanced_split_text(text, chunk_size=800, chunk_overlap=200)\n",
    "    print(f\"   문서 분할 완료: {len(text_chunks)}개 청크 생성\\n\")\n",
    "    \n",
    "    # 3. 임베딩 모델 초기화\n",
    "    print(\"3. 임베딩 모델 초기화...\")\n",
    "    embedding_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "    print(\"   임베딩 모델 초기화 완료\\n\")\n",
    "    \n",
    "    # 4. 벡터 저장소 생성\n",
    "    print(\"4. 벡터 저장소 생성 중...\")\n",
    "    vector_store, documents = create_vector_store(text_chunks, embedding_model)\n",
    "    print(\"    벡터 저장소 생성 완료\\n\")\n",
    "    \n",
    "    # 5. 질의 실행\n",
    "    print(\"5. 질의 실행 중...\")\n",
    "    query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 컨텍스트 품질 평가\n",
    "context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "\n",
    "# 7. 결과 출력\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" AI의 답변:\")\n",
    "print(\"=\"*60)\n",
    "print(results[\"answer\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" 검색 결과 요약:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"• 참고한 문서 조각 수: {len(results['source_documents'])}개\")\n",
    "print(f\"• 컨텍스트 품질 점수: {context_quality:.2f}/1.00\")\n",
    "print(f\"• 총 컨텍스트 길이: {len(results['context_used']):,} 문자\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📄 참고한 문서 미리보기:\")\n",
    "print(\"=\"*60)\n",
    "for i, doc in enumerate(results[\"source_documents\"][:3]):  # 상위 3개만 미리보기\n",
    "    preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "    print(f\"\\n[문서 {i+1}] {preview}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  개선된 Source Level2\n",
    "* Prompt 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. 환경 변수 로드 및 API 키 설정\n",
    "load_dotenv()\n",
    "\n",
    "#  2. OpenAI API 키 확인\n",
    "OPENAI_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"UPSTAGE API 키가 설정되지 않았습니다. .env 파일을 확인하세요.\")\n",
    "\n",
    "# 2. 한국어 법률 문서 전용 텍스트 전처리 함수\n",
    "def preprocess_korean_legal_text(text):\n",
    "    \"\"\"\n",
    "    한국어 법률 문서의 구조를 고려한 텍스트 전처리 함수\n",
    "    \n",
    "    Args:\n",
    "        text (str): 원본 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        str: 전처리된 텍스트\n",
    "        \n",
    "    주요 처리 내용:\n",
    "        - 불필요한 공백 및 개행 정리\n",
    "        - 법조문 번호 정규화 (제1조, 제2조 등)\n",
    "        - 항 번호를 아라비아 숫자로 변환 (①→제1항)\n",
    "        - 호 번호 정규화\n",
    "    \"\"\"\n",
    "    # 연속된 공백을 하나의 공백으로 통일\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 조항 번호 정규화: \"제 1 조\" -> \"제1조\" 형태로 통일\n",
    "    text = re.sub(r'제(\\d+)조', r'제\\1조', text)\n",
    "    \n",
    "    # 원문자 항 번호를 아라비아 숫자로 변환하여 검색 정확도 향상\n",
    "    # ①②③④⑤⑥⑦⑧⑨⑩ -> 제1항, 제2항, ... 제10항\n",
    "    text = re.sub(r'①|②|③|④|⑤|⑥|⑦|⑧|⑨|⑩', \n",
    "                  lambda m: f\"제{ord(m.group()) - ord('①') + 1}항\", text)\n",
    "    \n",
    "    # 호 번호 정규화: \"1. \" -> \"제1호 \" 형태로 변환\n",
    "    text = re.sub(r'(\\d+)\\.\\s', r'제\\1호 ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# 3. 법률 문서에 최적화된 텍스트 분할 함수\n",
    "def advanced_split_text(text, chunk_size=600, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    법률 문서의 구조적 특성을 고려한 지능적 텍스트 분할\n",
    "    \n",
    "    Args:\n",
    "        text (str): 분할할 텍스트\n",
    "        chunk_size (int): 각 청크의 목표 크기 (문자 수)\n",
    "        chunk_overlap (int): 청크 간 중복되는 문자 수\n",
    "        \n",
    "    Returns:\n",
    "        list: 분할된 텍스트 청크들의 리스트\n",
    "        \n",
    "    특징:\n",
    "        - 법률 문서의 계층 구조(조>항>호>목)를 고려한 분할 우선순위\n",
    "        - 의미적 완성도를 유지하면서 분할\n",
    "        - 토큰 한도를 고려한 적절한 크기 설정\n",
    "    \"\"\"\n",
    "    # 텍스트 전처리 수행\n",
    "    text = preprocess_korean_legal_text(text)\n",
    "    \n",
    "    # 법률 문서 구조를 고려한 분할 구분자들을 우선순위대로 설정\n",
    "    # 상위 구조부터 하위 구조 순서로 분할을 시도\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n제\", \"\\n**제\",  # 조항 단위 분할 (가장 우선)\n",
    "            \"\\n①\", \"\\n②\", \"\\n③\", \"\\n④\", \"\\n⑤\",  # 항 단위 분할\n",
    "            \"\\n1.\", \"\\n2.\", \"\\n3.\", \"\\n4.\", \"\\n5.\",  # 호 단위 분할\n",
    "            \"\\n가.\", \"\\n나.\", \"\\n다.\", \"\\n라.\", \"\\n마.\",  # 목 단위 분할\n",
    "            \"\\n\\n\",  # 문단 단위 분할\n",
    "            \"\\n\",    # 줄 단위 분할\n",
    "            \". \",    # 문장 단위 분할\n",
    "            \" \",     # 단어 단위 분할\n",
    "            \"\"       # 문자 단위 분할 (최후 수단)\n",
    "        ]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# 4. DOCX 파일 로딩 및 전처리 함수\n",
    "def load_docx_advanced(file_path):\n",
    "    \"\"\"\n",
    "    DOCX 파일을 로드하고 기본적인 텍스트 정리를 수행\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): DOCX 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        str: 정리된 텍스트\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: 파일 로딩 실패 시\n",
    "        ValueError: 텍스트 추출 실패 시\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Docx2txtLoader를 사용하여 DOCX 파일에서 텍스트 추출\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        \n",
    "        # 텍스트가 비어있는지 확인\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"문서에서 텍스트를 추출할 수 없습니다. 파일이 비어있거나 손상되었을 수 있습니다.\")\n",
    "        \n",
    "        # 기본적인 텍스트 정리 작업\n",
    "        # 연속된 빈 줄을 두 개의 줄바꿈으로 통일\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        # 연속된 공백과 탭을 하나의 공백으로 통일\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"문서 로딩 실패: {str(e)}\")\n",
    "\n",
    "# 5. 배치 처리를 통한 벡터 저장소 생성 함수\n",
    "def create_vector_store(text_chunks, embedding_model, batch_size=30):\n",
    "    \"\"\"\n",
    "    텍스트 청크들을 배치 단위로 처리하여 벡터 저장소 생성\n",
    "    토큰 한도 초과 문제를 해결하기 위해 배치 처리 방식 적용\n",
    "    \n",
    "    Args:\n",
    "        text_chunks (list): 분할된 텍스트 청크들\n",
    "        embedding_model: OpenAI 임베딩 모델 객체\n",
    "        batch_size (int): 한 번에 처리할 청크 수 (기본값: 30)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (FAISS 벡터 저장소, Document 객체들의 리스트)\n",
    "        \n",
    "    처리 과정:\n",
    "        1. 각 청크에 메타데이터 추가 (ID, 길이, 조항 정보 등)\n",
    "        2. 청크 크기가 너무 큰 경우 자동으로 제한\n",
    "        3. 배치 단위로 임베딩 생성하여 토큰 한도 문제 방지\n",
    "        4. FAISS merge 기능을 활용하여 배치별 결과 통합\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"   총 {len(text_chunks)}개 청크를 {batch_size}개씩 배치 처리...\")\n",
    "        \n",
    "        # Document 객체들을 저장할 리스트 초기화\n",
    "        documents = []\n",
    "        \n",
    "        # 각 텍스트 청크를 Document 객체로 변환하면서 메타데이터 추가\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            # 청크 크기가 너무 큰 경우 제한 (토큰 한도 방지)\n",
    "            if len(chunk) > 2000:\n",
    "                chunk = chunk[:2000] + \"...\"\n",
    "                print(f\"   경고: 청크 {i}가 너무 커서 2000자로 제한했습니다.\")\n",
    "            \n",
    "            # 각 청크에 추가할 메타데이터 구성\n",
    "            metadata = {\n",
    "                'chunk_id': i,  # 청크 고유 번호\n",
    "                'chunk_length': len(chunk),  # 청크 길이\n",
    "                'chunk_type': 'legal_document'  # 문서 유형\n",
    "            }\n",
    "            \n",
    "            # 청크 내용에서 조항 정보 자동 추출\n",
    "            # \"제n조\" 패턴을 찾아 메타데이터에 추가\n",
    "            if '제' in chunk and '조' in chunk:\n",
    "                article_match = re.search(r'제(\\d+)조', chunk)\n",
    "                if article_match:\n",
    "                    metadata['article'] = f\"제{article_match.group(1)}조\"\n",
    "            \n",
    "            # Document 객체 생성하여 리스트에 추가\n",
    "            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "        \n",
    "        # 배치별 벡터 저장소 생성 및 병합 과정\n",
    "        vector_store = None\n",
    "        total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "        \n",
    "        # 문서들을 배치 크기만큼 나누어 처리\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch_docs = documents[i:i + batch_size]\n",
    "            batch_num = (i // batch_size) + 1\n",
    "            \n",
    "            print(f\"   배치 {batch_num}/{total_batches} 처리 중... ({len(batch_docs)}개 문서)\")\n",
    "            \n",
    "            # 첫 번째 배치인 경우 새로운 벡터 저장소 생성\n",
    "            if vector_store is None:\n",
    "                vector_store = FAISS.from_documents(batch_docs, embedding_model)\n",
    "            else:\n",
    "                # 이후 배치들은 기존 벡터 저장소에 병합\n",
    "                batch_vector_store = FAISS.from_documents(batch_docs, embedding_model)\n",
    "                vector_store.merge_from(batch_vector_store)\n",
    "        \n",
    "        print(\"   모든 배치 처리 완료\")\n",
    "        return vector_store, documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"벡터 저장소 생성 실패: {str(e)}\")\n",
    "\n",
    "# 6. 키워드 기반 검색 함수\n",
    "def keyword_search(query, documents, k=5):\n",
    "    \"\"\"\n",
    "    단순한 키워드 매칭을 통한 문서 검색\n",
    "    벡터 검색과 상호 보완적으로 사용하여 검색 정확도 향상\n",
    "    \n",
    "    Args:\n",
    "        query (str): 검색 질의\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        k (int): 반환할 상위 문서 수\n",
    "        \n",
    "    Returns:\n",
    "        list: 관련도 순으로 정렬된 Document 객체들\n",
    "        \n",
    "    검색 로직:\n",
    "        1. 질의와 문서의 단어 교집합 계산\n",
    "        2. 교집합 크기를 질의 단어 수로 나누어 정규화\n",
    "        3. 정확한 구문 매칭 시 보너스 점수 부여\n",
    "        4. 점수 기준으로 상위 k개 문서 반환\n",
    "    \"\"\"\n",
    "    # 질의를 소문자로 변환하고 단어 단위로 분할\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # 각 문서의 점수를 계산할 리스트\n",
    "    scores = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        # 문서 내용을 소문자로 변환하고 단어 단위로 분할\n",
    "        content_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 질의 단어와 문서 단어의 교집합 계산\n",
    "        intersection = query_words.intersection(content_words)\n",
    "        \n",
    "        # 기본 점수: 교집합 크기를 질의 단어 수로 나누어 정규화 (0~1 범위)\n",
    "        score = len(intersection) / len(query_words) if query_words else 0\n",
    "        \n",
    "        # 보너스 점수: 질의 전체가 문서에 정확히 포함된 경우\n",
    "        if query.lower() in doc.page_content.lower():\n",
    "            score += 0.5\n",
    "        \n",
    "        # (점수, 인덱스, 문서) 튜플로 저장\n",
    "        scores.append((score, i, doc))\n",
    "    \n",
    "    # 점수 기준으로 내림차순 정렬하여 상위 k개 반환\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, _, doc in scores[:k]]\n",
    "\n",
    "# 7. 하이브리드 검색 함수 (벡터 + 키워드 검색 결합)\n",
    "def hybrid_search(query, vector_store, documents, k=5, alpha=0.7):\n",
    "    \"\"\"\n",
    "    벡터 유사도 검색과 키워드 검색을 결합한 하이브리드 검색\n",
    "    두 검색 방법의 장점을 결합하여 더 정확한 검색 결과 제공\n",
    "    \n",
    "    Args:\n",
    "        query (str): 검색 질의\n",
    "        vector_store: FAISS 벡터 저장소\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        k (int): 최종 반환할 문서 수\n",
    "        alpha (float): 벡터 검색 가중치 (0~1, 높을수록 벡터 검색 중시)\n",
    "        \n",
    "    Returns:\n",
    "        list: 종합 점수로 정렬된 상위 k개 Document 객체들\n",
    "        \n",
    "    검색 과정:\n",
    "        1. 벡터 유사도 검색으로 의미적으로 관련된 문서 찾기\n",
    "        2. 키워드 검색으로 정확한 용어 매칭 문서 찾기\n",
    "        3. 두 결과를 alpha 가중치로 결합\n",
    "        4. 중복 문서 처리 및 최종 점수 계산\n",
    "        5. 점수 순으로 정렬하여 상위 k개 반환\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 벡터 유사도 검색 수행\n",
    "    # 더 많은 후보를 가져와서 다양성 확보 (k*2개)\n",
    "    vector_results = vector_store.similarity_search(query, k=k*2)\n",
    "    \n",
    "    # 2. 키워드 기반 검색 수행\n",
    "    keyword_results = keyword_search(query, documents, k=k*2)\n",
    "    \n",
    "    # 3. 두 검색 결과를 점수와 함께 통합\n",
    "    combined_results = {}\n",
    "    \n",
    "    # 벡터 검색 결과에 점수 부여 (alpha 가중치 적용)\n",
    "    for i, doc in enumerate(vector_results):\n",
    "        # 문서 내용을 고유 키로 사용\n",
    "        doc_id = doc.page_content\n",
    "        # 순위가 높을수록 높은 점수 (1.0에서 시작하여 순위에 따라 감소)\n",
    "        vector_score = alpha * (1.0 - i / len(vector_results))\n",
    "        \n",
    "        combined_results[doc_id] = {\n",
    "            'document': doc,\n",
    "            'score': vector_score,\n",
    "            'vector_rank': i + 1,\n",
    "            'keyword_rank': None\n",
    "        }\n",
    "    \n",
    "    # 키워드 검색 결과에 점수 부여 ((1-alpha) 가중치 적용)\n",
    "    for i, doc in enumerate(keyword_results):\n",
    "        doc_id = doc.page_content\n",
    "        keyword_score = (1 - alpha) * (1.0 - i / len(keyword_results))\n",
    "        \n",
    "        if doc_id in combined_results:\n",
    "            # 이미 벡터 검색에서 찾은 문서인 경우 점수 합산\n",
    "            combined_results[doc_id]['score'] += keyword_score\n",
    "            combined_results[doc_id]['keyword_rank'] = i + 1\n",
    "        else:\n",
    "            # 키워드 검색에서만 찾은 새로운 문서인 경우 추가\n",
    "            combined_results[doc_id] = {\n",
    "                'document': doc,\n",
    "                'score': keyword_score,\n",
    "                'vector_rank': None,\n",
    "                'keyword_rank': i + 1\n",
    "            }\n",
    "    \n",
    "    # 4. 종합 점수 기준으로 정렬하여 상위 k개 반환\n",
    "    sorted_results = sorted(combined_results.values(), key=lambda x: x['score'], reverse=True)\n",
    "    return [result['document'] for result in sorted_results[:k]]\n",
    "\n",
    "# 8. 한국어 법률 문서 전용 프롬프트 생성 함수\n",
    "def create_korean_legal_prompt():\n",
    "    \"\"\"\n",
    "    한국어 법률 문서 특성에 맞춘 전용 프롬프트 템플릿 생성\n",
    "    \n",
    "    Returns:\n",
    "        PromptTemplate: 법률 문서 질의응답을 위한 프롬프트 템플릿\n",
    "        \n",
    "    프롬프트 특징:\n",
    "        - 법조문 인용의 정확성 강조\n",
    "        - 전문 용어에 대한 쉬운 설명 요구\n",
    "        - 조항 간 연관성 설명 포함\n",
    "        - 실무적 적용 방법 제시\n",
    "        - 불확실한 내용에 대한 명시적 언급\n",
    "    \"\"\"\n",
    "    template = \"\"\"당신은 한국 세법 전문가입니다. 주어진 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해야 합니다.\n",
    "\n",
    "다음 규칙을 반드시 따르세요:\n",
    "1. 법조문의 조항, 항, 호, 목을 정확히 인용하세요\n",
    "2. 전문 용어를 사용할 때는 쉬운 설명을 함께 제공하세요\n",
    "3. 관련 조항들 간의 연관성을 설명하세요\n",
    "4. 실무적 적용 방법도 함께 설명하세요\n",
    "5. 불확실한 내용이 있으면 명시적으로 언급하세요\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "위 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해주세요. 관련 조항을 인용하며 설명해주세요.\"\"\"\n",
    "\n",
    "    return PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "# 9. LLM을 활용한 질문 응답 함수\n",
    "def query_with_llm(query, vector_store, documents):\n",
    "    \"\"\"\n",
    "    하이브리드 검색과 고성능 LLM을 결합한 질문 응답 시스템\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        vector_store: FAISS 벡터 저장소\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        \n",
    "    Returns:\n",
    "        dict: 답변, 참고 문서, 사용된 컨텍스트를 포함한 응답 딕셔너리\n",
    "        \n",
    "    처리 과정:\n",
    "        1. GPT-4o-mini 모델로 LLM 초기화 (높은 정확도)\n",
    "        2. 하이브리드 검색으로 관련 문서 7개 검색\n",
    "        3. 검색된 문서들을 하나의 컨텍스트로 결합\n",
    "        4. 법률 문서 전용 프롬프트 적용\n",
    "        5. LLM으로 최종 답변 생성\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # LLM 모델 설정\n",
    "        llm = ChatUpstage(\n",
    "            model=\"solar-pro\",\n",
    "            base_url=\"https://api.upstage.ai/v1\",\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        print(f\"질의: {query}\")\n",
    "        print(\"하이브리드 검색 수행 중...\")\n",
    "        \n",
    "        # 하이브리드 검색으로 관련 문서 검색\n",
    "        # k=7로 설정하여 충분한 컨텍스트 확보\n",
    "        # alpha=0.7로 설정하여 벡터 검색을 더 중시 (의미적 유사도 우선)\n",
    "        relevant_docs = hybrid_search(query, vector_store, documents, k=7, alpha=0.7)\n",
    "        \n",
    "        print(f\"검색된 관련 문서: {len(relevant_docs)}개\")\n",
    "        \n",
    "        # 검색된 문서들을 하나의 컨텍스트로 결합\n",
    "        # 각 문서에 번호를 매겨 구분하기 쉽게 구성\n",
    "        context = \"\\n\\n\".join([f\"[문서 {i+1}]\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # 한국어 법률 문서에 특화된 프롬프트 사용\n",
    "        prompt = create_korean_legal_prompt()\n",
    "        \n",
    "        # 최종 프롬프트 생성 (컨텍스트와 질문 삽입)\n",
    "        formatted_prompt = prompt.format(context=context, question=query)\n",
    "        \n",
    "        print(\"LLM 응답 생성 중...\")\n",
    "        \n",
    "        # LLM에 프롬프트 전달하여 답변 생성\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        # 결과를 딕셔너리 형태로 반환\n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"source_documents\": relevant_docs,\n",
    "            \"context_used\": context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM 응답 생성 실패: {str(e)}\")\n",
    "\n",
    "# 10. 검색 품질 평가 함수\n",
    "def evaluate_context_quality(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    검색된 문서들의 품질을 정량적으로 평가\n",
    "    \n",
    "    Args:\n",
    "        query (str): 원본 질의\n",
    "        retrieved_docs (list): 검색된 Document 객체들\n",
    "        \n",
    "    Returns:\n",
    "        float: 0~1 범위의 품질 점수 (1에 가까울수록 높은 품질)\n",
    "        \n",
    "    평가 기준:\n",
    "        1. 키워드 매칭률 (70% 가중치): 질의 단어가 문서에 포함된 비율\n",
    "        2. 문서 길이 적절성 (30% 가중치): 너무 짧거나 길지 않은 적절한 길이\n",
    "    \"\"\"\n",
    "    # 질의를 단어 단위로 분할하고 소문자 변환\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # 각 문서별 품질 점수 계산\n",
    "    quality_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        # 문서 내용을 단어 단위로 분할하고 소문자 변환\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 키워드 매칭 점수 계산\n",
    "        # 질의 단어 중 문서에 포함된 단어의 비율\n",
    "        keyword_match = len(query_words.intersection(doc_words)) / len(query_words)\n",
    "        \n",
    "        # 문서 길이 점수 계산\n",
    "        # 1000자를 기준으로 정규화 (1000자 이상이면 1.0점)\n",
    "        length_score = min(len(doc.page_content) / 1000, 1.0)\n",
    "        \n",
    "        # 종합 점수 계산 (키워드 매칭 70% + 길이 적절성 30%)\n",
    "        total_score = (keyword_match * 0.7) + (length_score * 0.3)\n",
    "        quality_scores.append(total_score)\n",
    "    \n",
    "    # 전체 문서의 평균 품질 점수 반환\n",
    "    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n",
    "    return avg_quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 메인 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    # 처리할 DOCX 파일 경로 설정\n",
    "    docx_path = \"data/tax_with_table.docx\"\n",
    "    \n",
    "    print(\"개선된 RAG 파이프라인 실행\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1단계: 문서 로드\n",
    "    print(\"1. 문서 로드 중...\")\n",
    "    text = load_docx_advanced(docx_path)\n",
    "    print(f\"   문서 로드 완료: {len(text):,} 문자\")\n",
    "    \n",
    "    # 2단계: 문서 분할\n",
    "    print(\"\\n2. 문서 분할 중...\")\n",
    "    text_chunks = advanced_split_text(text, chunk_size=600, chunk_overlap=100)\n",
    "    print(f\"   문서 분할 완료: {len(text_chunks)}개 청크 생성\")\n",
    "    \n",
    "    # 청크 크기 통계 분석 및 출력\n",
    "    chunk_lengths = [len(chunk) for chunk in text_chunks]\n",
    "    avg_length = sum(chunk_lengths) / len(chunk_lengths)\n",
    "    max_length = max(chunk_lengths)\n",
    "    print(f\"   평균 청크 길이: {avg_length:.0f}자, 최대 길이: {max_length}자\")\n",
    "    \n",
    "    # 3단계: 임베딩 모델 초기화\n",
    "    print(\"\\n3. 임베딩 모델 초기화...\")\n",
    "    \n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-small\",\n",
    "    )\n",
    "    print(\"   임베딩 모델 초기화 완료\")\n",
    "    \n",
    "    # 4단계: 벡터 저장소 생성\n",
    "    print(\"\\n4. 벡터 저장소 생성 중...\")\n",
    "    # 배치 크기 30으로 설정하여 토큰 한도 문제 방지\n",
    "    vector_store, documents = create_vector_store(text_chunks, embedding_model, batch_size=30)\n",
    "    print(\"   벡터 저장소 생성 완료\")\n",
    "    \n",
    "    # 5단계: 질의 실행\n",
    "    print(\"\\n5. 질의 실행 중...\")\n",
    "   #query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    query = \"비과세소득의 종류에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    "    \n",
    "    # 6단계: 검색 품질 평가\n",
    "    context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "    \n",
    "    # 7단계: 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AI의 답변:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results[\"answer\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"검색 결과 요약:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"참고한 문서 조각 수: {len(results['source_documents'])}개\")\n",
    "    print(f\"컨텍스트 품질 점수: {context_quality:.2f}/1.00\")\n",
    "    print(f\"총 컨텍스트 길이: {len(results['context_used']):,} 문자\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"참고한 문서 미리보기:\")\n",
    "    print(\"=\" * 60)\n",
    "    # 상위 3개 문서의 일부만 미리보기로 출력\n",
    "    for i, doc in enumerate(results[\"source_documents\"][:3]):\n",
    "        # 200자까지만 미리보기로 표시\n",
    "        preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "        print(f\"\\n[문서 {i+1}] {preview}\")\n",
    "            # 5단계: 질의 실행\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n5. 질의 실행 중...\")\n",
    "    query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    "    \n",
    "    # 6단계: 검색 품질 평가\n",
    "    context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "    \n",
    "    # 7단계: 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AI의 답변:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results[\"answer\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"검색 결과 요약:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"참고한 문서 조각 수: {len(results['source_documents'])}개\")\n",
    "    print(f\"컨텍스트 품질 점수: {context_quality:.2f}/1.00\")\n",
    "    print(f\"총 컨텍스트 길이: {len(results['context_used']):,} 문자\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"참고한 문서 미리보기:\")\n",
    "    print(\"=\" * 60)\n",
    "    # 상위 3개 문서의 일부만 미리보기로 출력\n",
    "    for i, doc in enumerate(results[\"source_documents\"][:3]):\n",
    "        # 200자까지만 미리보기로 표시\n",
    "        preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "        print(f\"\\n[문서 {i+1}] {preview}\")\n",
    "    print(\"=\" * 60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
