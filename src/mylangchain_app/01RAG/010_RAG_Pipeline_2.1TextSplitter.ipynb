{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1F5lTDp5UPf0",
   "metadata": {
    "id": "1F5lTDp5UPf0"
   },
   "source": [
    "##### 1) 라이브러리 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55152049-e9e5-4952-8e19-409f58cf3ac9",
   "metadata": {
    "id": "55152049-e9e5-4952-8e19-409f58cf3ac9"
   },
   "source": [
    "##### 2) OpenAI 인증키 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a0e6609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-pr\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb011d",
   "metadata": {},
   "source": [
    "#### CharacterTextSplitter 간단한 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a7fe9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 84, which is longer than the specified 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1 LLM은 지능형 챗봇 및 기타 자연어 처리(NLP) 애플리케이션을 지원하는 핵심 인공 지능(AI) 기술입니다\n",
      "2 신뢰할 수 있는 지식 소스를 상호 참조하여 다양한 상황에서 사용자 질문에 답변할 수 있는 봇을 만드는 것이 LLM의 목표입니다\n",
      "3 안타깝게도 LLM 기술의 특성상 LLM 응답에 대한 예측이 불가능합니다\n",
      "4 또한 LLM 훈련 데이터는 정적이며 보유한 지식은 일정 기간 동안만 유용합니다.\n",
      "\n",
      "LLM의 알려진 문제점은 다음과 같습니다\n",
      "5 LLM의 알려진 문제점은 다음과 같습니다.\n",
      "\n",
      "답이 없을 때 허위 정보를 제공합니다\n",
      "6 답이 없을 때 허위 정보를 제공합니다.\n",
      "사용자가 구체적이고 최신의 응답을 기대할 때 오래되었거나 일반적인 정보를 제공합니다\n",
      "7 신뢰할 수 없는 출처로부터 응답을 생성합니다\n",
      "8 다양한 훈련 소스에서 동일한 용어를 사용하여 다른 내용을 설명하면서 용어 혼동으로 인해 응답이 부정확합니다\n",
      "9 대형 언어 모델은 현재 상황에 대한 최신 정보는 없지만 항상 절대적인 자신감을 가지고 모든 질문에 답변하는 열정적인 신입 사원으로 생각할 수 있습니다\n",
      "10 안타깝게도 이러한 태도는 사용자 신뢰에 부정적인 영향을 미칠 수 있으며 챗봇이 모방해서는 안 되는 것입니다\n",
      "11 RAG는 이러한 문제 중 일부를 해결하기 위한 접근 방식입니다\n",
      "12 LLM을 리디렉션하여 신뢰할 수 있는 사전 결정된 지식 출처에서 관련 정보를 검색합니다\n",
      "13 조직은 생성된 텍스트 출력을 더 잘 제어할 수 있으며 사용자는 LLM이 응답을 생성하는 방식에 대한 인사이트를 얻을 수 있습니다.\n",
      "중복발견 출력 CharacterTextSplitter ======================================================\n",
      "청크 1: 'LLM은 지능형 챗봇 및 기타 자연어 처리(NLP) 애플리케이션을 지원하는 핵심 인공 지능(AI) 기술입니다' (길이: 60자)\n",
      "청크 2: '신뢰할 수 있는 지식 소스를 상호 참조하여 다양한 상황에서 사용자 질문에 답변할 수 있는 봇을 만드는 것이 LLM의 목표입니다' (길이: 70자)\n",
      "청크 3: '안타깝게도 LLM 기술의 특성상 LLM 응답에 대한 예측이 불가능합니다' (길이: 39자)\n",
      "청크 4: '또한 LLM 훈련 데이터는 정적이며 보유한 지식은 일정 기간 동안만 유용합니다.\n",
      "\n",
      "LLM의 알려진 문제점은 다음과 같습니다' (길이: 68자)\n",
      "청크 5: 'LLM의 알려진 문제점은 다음과 같습니다.\n",
      "\n",
      "답이 없을 때 허위 정보를 제공합니다' (길이: 45자)\n",
      "   → 중복 발견: 'M의 알려진 문제점은 다음과 같습니다'\n",
      "청크 6: '답이 없을 때 허위 정보를 제공합니다.\n",
      "사용자가 구체적이고 최신의 응답을 기대할 때 오래되었거나 일반적인 정보를 제공합니다' (길이: 68자)\n",
      "   → 중복 발견: '답이 없을 때 허위 정보를 제공합니다'\n",
      "청크 7: '신뢰할 수 없는 출처로부터 응답을 생성합니다' (길이: 24자)\n",
      "청크 8: '다양한 훈련 소스에서 동일한 용어를 사용하여 다른 내용을 설명하면서 용어 혼동으로 인해 응답이 부정확합니다' (길이: 59자)\n",
      "청크 9: '대형 언어 모델은 현재 상황에 대한 최신 정보는 없지만 항상 절대적인 자신감을 가지고 모든 질문에 답변하는 열정적인 신입 사원으로 생각할 수 있습니다' (길이: 83자)\n",
      "청크 10: '안타깝게도 이러한 태도는 사용자 신뢰에 부정적인 영향을 미칠 수 있으며 챗봇이 모방해서는 안 되는 것입니다' (길이: 59자)\n",
      "청크 11: 'RAG는 이러한 문제 중 일부를 해결하기 위한 접근 방식입니다' (길이: 34자)\n",
      "청크 12: 'LLM을 리디렉션하여 신뢰할 수 있는 사전 결정된 지식 출처에서 관련 정보를 검색합니다' (길이: 48자)\n",
      "청크 13: '조직은 생성된 텍스트 출력을 더 잘 제어할 수 있으며 사용자는 LLM이 응답을 생성하는 방식에 대한 인사이트를 얻을 수 있습니다.' (길이: 72자)\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 원본 텍스트\n",
    "text = \"\"\"\n",
    "LLM은 지능형 챗봇 및 기타 자연어 처리(NLP) 애플리케이션을 지원하는 핵심 인공 지능(AI) 기술입니다. \n",
    "신뢰할 수 있는 지식 소스를 상호 참조하여 다양한 상황에서 사용자 질문에 답변할 수 있는 봇을 만드는 것이 LLM의 목표입니다. \n",
    "안타깝게도 LLM 기술의 특성상 LLM 응답에 대한 예측이 불가능합니다. 또한 LLM 훈련 데이터는 정적이며 보유한 지식은 일정 기간 동안만 유용합니다.\n",
    "\n",
    "LLM의 알려진 문제점은 다음과 같습니다.\n",
    "\n",
    "답이 없을 때 허위 정보를 제공합니다.\n",
    "사용자가 구체적이고 최신의 응답을 기대할 때 오래되었거나 일반적인 정보를 제공합니다.\n",
    "신뢰할 수 없는 출처로부터 응답을 생성합니다.\n",
    "다양한 훈련 소스에서 동일한 용어를 사용하여 다른 내용을 설명하면서 용어 혼동으로 인해 응답이 부정확합니다.\n",
    "대형 언어 모델은 현재 상황에 대한 최신 정보는 없지만 항상 절대적인 자신감을 가지고 모든 질문에 답변하는 열정적인 신입 사원으로 생각할 수 있습니다. \n",
    "안타깝게도 이러한 태도는 사용자 신뢰에 부정적인 영향을 미칠 수 있으며 챗봇이 모방해서는 안 되는 것입니다.\n",
    "\n",
    "RAG는 이러한 문제 중 일부를 해결하기 위한 접근 방식입니다. LLM을 리디렉션하여 신뢰할 수 있는 사전 결정된 지식 출처에서 관련 정보를 검색합니다. \n",
    "조직은 생성된 텍스트 출력을 더 잘 제어할 수 있으며 사용자는 LLM이 응답을 생성하는 방식에 대한 인사이트를 얻을 수 있습니다.\n",
    "\"\"\"\n",
    "\n",
    "# 마침표(\".\")를 기준으로 텍스트 분할\n",
    "splitter = CharacterTextSplitter(chunk_size=80, chunk_overlap=30, separator=\".\")\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "print(type(chunks))\n",
    "#print(chunks)\n",
    "for idx,chunk in enumerate(chunks,1):\n",
    "    print(idx, chunk)\n",
    "\n",
    "print('중복발견 출력 CharacterTextSplitter ======================================================')\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"청크 {i}: '{chunk}' (길이: {len(chunk)}자)\")\n",
    "    \n",
    "    # 중복 확인\n",
    "    if i > 1:\n",
    "        prev_chunk = chunks[i-2]\n",
    "        curr_chunk = chunk\n",
    "        \n",
    "        # 간단한 중복 문자열 찾기\n",
    "        for length in range(20, 5, -1):  # 20자부터 5자까지 중복 찾기\n",
    "            if len(prev_chunk) >= length and len(curr_chunk) >= length:\n",
    "                prev_end = prev_chunk[-length:]\n",
    "                if prev_end in curr_chunk:\n",
    "                    print(f\"   → 중복 발견: '{prev_end}'\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3266ca",
   "metadata": {},
   "source": [
    "#####  CharacterTextSplitter 대신 RecursiveCharacterTextSplitter 사용함\n",
    "* RecursiveCharacterTextSplitter 장점:\n",
    "    * 여러 separator를 순서대로 시도\n",
    "    * 자연스러운 경계에서 분할\n",
    "    * chunk_size와 overlap이 더 정확하게 작동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f00233a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RecursiveCharacterTextSplitter 사용 (권장):\n",
      "생성된 청크 수: 15개\n",
      "1 LLM은 지능형 챗봇 및 기타 자연어 처리(NLP) 애플리케이션을 지원하는 핵심 인공 지능(AI) 기술입니다.\n",
      "2 신뢰할 수 있는 지식 소스를 상호 참조하여 다양한 상황에서 사용자 질문에 답변할 수 있는 봇을 만드는 것이 LLM의 목표입니다.\n",
      "3 안타깝게도 LLM 기술의 특성상 LLM 응답에 대한 예측이 불가능합니다\n",
      "4 . 또한 LLM 훈련 데이터는 정적이며 보유한 지식은 일정 기간 동안만 유용합니다.\n",
      "5 LLM의 알려진 문제점은 다음과 같습니다.\n",
      "6 답이 없을 때 허위 정보를 제공합니다.\n",
      "사용자가 구체적이고 최신의 응답을 기대할 때 오래되었거나 일반적인 정보를 제공합니다.\n",
      "7 신뢰할 수 없는 출처로부터 응답을 생성합니다.\n",
      "8 다양한 훈련 소스에서 동일한 용어를 사용하여 다른 내용을 설명하면서 용어 혼동으로 인해 응답이 부정확합니다.\n",
      "9 대형 언어 모델은 현재 상황에 대한 최신 정보는 없지만 항상 절대적인 자신감을 가지고 모든 질문에 답변하는 열정적인 신입 사원으로 생각할 수\n",
      "10 질문에 답변하는 열정적인 신입 사원으로 생각할 수 있습니다\n",
      "11 .\n",
      "12 안타깝게도 이러한 태도는 사용자 신뢰에 부정적인 영향을 미칠 수 있으며 챗봇이 모방해서는 안 되는 것입니다.\n",
      "13 RAG는 이러한 문제 중 일부를 해결하기 위한 접근 방식입니다\n",
      "14 . LLM을 리디렉션하여 신뢰할 수 있는 사전 결정된 지식 출처에서 관련 정보를 검색합니다.\n",
      "15 조직은 생성된 텍스트 출력을 더 잘 제어할 수 있으며 사용자는 LLM이 응답을 생성하는 방식에 대한 인사이트를 얻을 수 있습니다.\n",
      "중복발견 출력 RecursiveCharacterTextSplitter ======================================================\n",
      "청크 1: 'LLM은 지능형 챗봇 및 기타 자연어 처리(NLP) 애플리케이션을 지원하는 핵심 인공 지능(AI) 기술입니다.' (길이: 61자)\n",
      "청크 2: '신뢰할 수 있는 지식 소스를 상호 참조하여 다양한 상황에서 사용자 질문에 답변할 수 있는 봇을 만드는 것이 LLM의 목표입니다.' (길이: 71자)\n",
      "청크 3: '안타깝게도 LLM 기술의 특성상 LLM 응답에 대한 예측이 불가능합니다' (길이: 39자)\n",
      "청크 4: '. 또한 LLM 훈련 데이터는 정적이며 보유한 지식은 일정 기간 동안만 유용합니다.' (길이: 46자)\n",
      "청크 5: 'LLM의 알려진 문제점은 다음과 같습니다.' (길이: 23자)\n",
      "청크 6: '답이 없을 때 허위 정보를 제공합니다.\n",
      "사용자가 구체적이고 최신의 응답을 기대할 때 오래되었거나 일반적인 정보를 제공합니다.' (길이: 69자)\n",
      "청크 7: '신뢰할 수 없는 출처로부터 응답을 생성합니다.' (길이: 25자)\n",
      "청크 8: '다양한 훈련 소스에서 동일한 용어를 사용하여 다른 내용을 설명하면서 용어 혼동으로 인해 응답이 부정확합니다.' (길이: 60자)\n",
      "청크 9: '대형 언어 모델은 현재 상황에 대한 최신 정보는 없지만 항상 절대적인 자신감을 가지고 모든 질문에 답변하는 열정적인 신입 사원으로 생각할 수' (길이: 78자)\n",
      "청크 10: '질문에 답변하는 열정적인 신입 사원으로 생각할 수 있습니다' (길이: 32자)\n",
      "   → 중복 발견: '는 열정적인 신입 사원으로 생각할 수'\n",
      "청크 11: '.' (길이: 1자)\n",
      "청크 12: '안타깝게도 이러한 태도는 사용자 신뢰에 부정적인 영향을 미칠 수 있으며 챗봇이 모방해서는 안 되는 것입니다.' (길이: 60자)\n",
      "청크 13: 'RAG는 이러한 문제 중 일부를 해결하기 위한 접근 방식입니다' (길이: 34자)\n",
      "청크 14: '. LLM을 리디렉션하여 신뢰할 수 있는 사전 결정된 지식 출처에서 관련 정보를 검색합니다.' (길이: 51자)\n",
      "청크 15: '조직은 생성된 텍스트 출력을 더 잘 제어할 수 있으며 사용자는 LLM이 응답을 생성하는 방식에 대한 인사이트를 얻을 수 있습니다.' (길이: 72자)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 원본 텍스트를 RecursiveCharacterTextSplitter로 처리\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"\"\"\n",
    "LLM은 지능형 챗봇 및 기타 자연어 처리(NLP) 애플리케이션을 지원하는 핵심 인공 지능(AI) 기술입니다. \n",
    "신뢰할 수 있는 지식 소스를 상호 참조하여 다양한 상황에서 사용자 질문에 답변할 수 있는 봇을 만드는 것이 LLM의 목표입니다. \n",
    "안타깝게도 LLM 기술의 특성상 LLM 응답에 대한 예측이 불가능합니다. 또한 LLM 훈련 데이터는 정적이며 보유한 지식은 일정 기간 동안만 유용합니다.\n",
    "\n",
    "LLM의 알려진 문제점은 다음과 같습니다.\n",
    "\n",
    "답이 없을 때 허위 정보를 제공합니다.\n",
    "사용자가 구체적이고 최신의 응답을 기대할 때 오래되었거나 일반적인 정보를 제공합니다.\n",
    "신뢰할 수 없는 출처로부터 응답을 생성합니다.\n",
    "다양한 훈련 소스에서 동일한 용어를 사용하여 다른 내용을 설명하면서 용어 혼동으로 인해 응답이 부정확합니다.\n",
    "대형 언어 모델은 현재 상황에 대한 최신 정보는 없지만 항상 절대적인 자신감을 가지고 모든 질문에 답변하는 열정적인 신입 사원으로 생각할 수 있습니다. \n",
    "안타깝게도 이러한 태도는 사용자 신뢰에 부정적인 영향을 미칠 수 있으며 챗봇이 모방해서는 안 되는 것입니다.\n",
    "\n",
    "RAG는 이러한 문제 중 일부를 해결하기 위한 접근 방식입니다. LLM을 리디렉션하여 신뢰할 수 있는 사전 결정된 지식 출처에서 관련 정보를 검색합니다. \n",
    "조직은 생성된 텍스트 출력을 더 잘 제어할 수 있으며 사용자는 LLM이 응답을 생성하는 방식에 대한 인사이트를 얻을 수 있습니다.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nRecursiveCharacterTextSplitter 사용 (권장):\")\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=30,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # 우선순위별 분할자\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_text(text)\n",
    "print(f\"생성된 청크 수: {len(recursive_chunks)}개\")\n",
    "\n",
    "for idx,chunk in enumerate(recursive_chunks,1):\n",
    "    print(idx, chunk)\n",
    "\n",
    "print('중복발견 출력 RecursiveCharacterTextSplitter ======================================================')\n",
    "for i, chunk in enumerate(recursive_chunks, 1):\n",
    "    print(f\"청크 {i}: '{chunk}' (길이: {len(chunk)}자)\")\n",
    "    \n",
    "    # 중복 확인\n",
    "    if i > 1:\n",
    "        prev_chunk = recursive_chunks[i-2]\n",
    "        curr_chunk = chunk\n",
    "        \n",
    "        # 간단한 중복 문자열 찾기\n",
    "        for length in range(20, 5, -1):  # 20자부터 5자까지 중복 찾기\n",
    "            if len(prev_chunk) >= length and len(curr_chunk) >= length:\n",
    "                prev_end = prev_chunk[-length:]\n",
    "                if prev_end in curr_chunk:\n",
    "                    print(f\"   → 중복 발견: '{prev_end}'\")\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf721ee",
   "metadata": {},
   "source": [
    "#### TokenTextSplitter 간단한 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3b1d6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 총 14개의 청크로 분할됨.\n",
      "\n",
      " 첫 번째 청크:\n",
      " Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대�\n",
      "\n",
      " Chunk 1 (길이: 270):\n",
      "Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대�\n",
      "\n",
      " Chunk 2 (길이: 229):\n",
      "성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
      "연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\n",
      "\n",
      "Embedding (임베딩)\n",
      "\n",
      "정의: 단어나 문장을 벡터 공간에 매핑하여 의미적으로 유사한 것들이 가까이 위치하도록 하는 기법.\n",
      "예시: \"강\n",
      "\n",
      " Chunk 3 (길이: 233):\n",
      "까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 유사하게 위치함.\n",
      "연관 키워드: 벡터화, 자연어 처리, 딥러닝\n",
      "\n",
      "Token (토큰)\n",
      "\n",
      "정의: 텍스트 데이터를 더 작은 단위(단어, 문자, 문장 등)로 나누는 과정.\n",
      "예시: \"AI는 혁신적이다\"를 [\"AI\", \"는\", \"혁신적\", \"이다\"]로 분할.\n",
      "연관 키워드: 토큰화, NLP, 텍스트 전처리\n",
      "\n",
      "Transformer (트랜스포머)\n",
      "\n",
      "정의: 자\n",
      "\n",
      " Chunk 4 (길이: 237):\n",
      "�리\n",
      "\n",
      "Transformer (트랜스포머)\n",
      "\n",
      "정의: 자연어 처리에서 사용되는 신경망 아키텍처로, 병렬 연산과 장기 의존성 처리가 강점.\n",
      "예시: GPT, BERT 등의 모델이 트랜스포머 기반으로 동작함.\n",
      "연관 키워드: 딥러닝, 자기 주의 메커니즘, NLP\n",
      "\n",
      "Self-Attention (자기 주의 메커니즘)\n",
      "\n",
      "정의: 문장의 모든 단어가 서로에게 가중치를 부여하여 문맥을 이해하는 방식.\n",
      "예시: \"나는 강아지를 좋아한다\"에서 \"\n",
      "\n",
      " Chunk 5 (길이: 237):\n",
      ".\n",
      "예시: \"나는 강아지를 좋아한다\"에서 \"나는\"과 \"좋아한다\"가 강한 연관성을 가짐.\n",
      "연관 키워드: 트랜스포머, BERT, 문맥 학습\n",
      "\n",
      "Fine-Tuning (미세 조정)\n",
      "\n",
      "정의: 사전 학습된 모델을 특정 작업에 맞게 추가 학습하는 과정.\n",
      "예시: GPT 모델을 법률 문서 요약에 맞게 학습.\n",
      "연관 키워드: 전이 학습, 모델 최적화, AI 응용\n",
      "\n",
      "Zero-shot Learning (제로샷 학습)\n",
      "\n",
      "정의: 특정 태스크에 대한\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# 파일 읽기\n",
    "with open(\"../data/ai-terminology.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일 내용을 읽어오기\n",
    "\n",
    "#print(\"원본 텍스트 미리보기:\\n\", file[:500])  # 앞 500자 출력\n",
    "\n",
    "# TokenTextSplitter 설정\n",
    "text_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200,  # 청크 크기\n",
    "    chunk_overlap=20,  # 청크 간 겹치는 부분 추가하여 문맥 유지\n",
    "    encoding_name=\"cl100k_base\",  # OpenAI tiktoken 기본 인코딩 사용 (한글 처리 개선)\n",
    "    add_start_index=True  # 각 청크의 시작 인덱스 반환\n",
    ")\n",
    "\n",
    "# 텍스트 분할 실행\n",
    "texts = text_splitter.split_text(file)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"\\n 총 {len(texts)}개의 청크로 분할됨.\")\n",
    "print(\"\\n 첫 번째 청크:\\n\", texts[0])\n",
    "\n",
    "# 청크 길이 확인\n",
    "for i, chunk in enumerate(texts[:5]):  # 처음 5개만 확인\n",
    "    print(f\"\\n Chunk {i+1} (길이: {len(chunk)}):\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edaab543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 읽기 성공: ../data/ai-terminology.txt\n",
      "원본 텍스트 미리보기:\n",
      "--------------------------------------------------\n",
      "\n",
      "전체 텍스트 길이: 3036자\n",
      "\n",
      "============================================================\n",
      "TokenTextSplitter 특징 및 RecursiveCharacterTextSplitter Splitter와 비교\n",
      "============================================================\n",
      "\n",
      "원본 텍스트의 토큰 개수: 2407개\n",
      "문자 대 토큰 비율: 1.26 (문자/토큰)\n",
      "\n",
      "1. TokenTextSplitter (토큰 기반 분할):\n",
      "----------------------------------------\n",
      "총 14개 청크 생성\n",
      "\n",
      "Chunk 1:\n",
      "  토큰 수: 200개\n",
      "  문자 수: 270자\n",
      "  내용: Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를...\n",
      "\n",
      "Chunk 2:\n",
      "  토큰 수: 200개\n",
      "  문자 수: 229자\n",
      "  내용: 성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 ...\n",
      "\n",
      "Chunk 3:\n",
      "  토큰 수: 200개\n",
      "  문자 수: 233자\n",
      "  내용: 까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 유사하게 위치함.\n",
      "연관 키워드: 벡터화, 자연어 처리, 딥러닝\n",
      "\n",
      "Token (토큰)\n",
      "\n",
      "정의: 텍스트 데이터...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# 파일 읽기\n",
    "file_path = \"../data/ai-terminology.txt\"\n",
    "\n",
    "# 파일 읽기\n",
    "try:\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        ai_terminology_text = f.read()\n",
    "    print(f\"파일 읽기 성공: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"파일 읽기 실패: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"원본 텍스트 미리보기:\")\n",
    "print(\"-\" * 50)\n",
    "#print(ai_terminology_text[:500] + \"...\")\n",
    "print(f\"\\n전체 텍스트 길이: {len(ai_terminology_text)}자\")\n",
    "\n",
    "# ===================================================================\n",
    "# TokenTextSplitter vs RecursiveCharacterTextSplitter Splitter 비교\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TokenTextSplitter 특징 및 RecursiveCharacterTextSplitter Splitter와 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 토큰 개수 확인 (tiktoken 사용)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens = encoding.encode(ai_terminology_text)\n",
    "print(f\"\\n원본 텍스트의 토큰 개수: {len(tokens)}개\")\n",
    "print(f\"문자 대 토큰 비율: {len(ai_terminology_text)/len(tokens):.2f} (문자/토큰)\")\n",
    "\n",
    "# 2. TokenTextSplitter 설정 및 실행\n",
    "print(\"\\n1. TokenTextSplitter (토큰 기반 분할):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "token_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200,              # 토큰 개수로 크기 지정\n",
    "    chunk_overlap=20,            # 토큰 단위 중복\n",
    "    encoding_name=\"cl100k_base\", # OpenAI GPT 모델용 인코딩\n",
    "    add_start_index=True         # 시작 인덱스 정보 포함\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(ai_terminology_text)\n",
    "print(f\"총 {len(token_chunks)}개 청크 생성\")\n",
    "\n",
    "for i, chunk in enumerate(token_chunks[:3], 1):\n",
    "    chunk_tokens = encoding.encode(chunk)\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  토큰 수: {len(chunk_tokens)}개\")\n",
    "    print(f\"  문자 수: {len(chunk)}자\")\n",
    "    print(f\"  내용: {chunk[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df5cba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. RecursiveCharacterTextSplitter (문자 기반 분할):\n",
      "---------------------------------------------\n",
      "총 5개 청크 생성\n",
      "\n",
      "Chunk 1:\n",
      "  토큰 수: 556개\n",
      "  문자 수: 685자\n",
      "  내용: Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를...\n",
      "\n",
      "Chunk 2:\n",
      "  토큰 수: 608개\n",
      "  문자 수: 730자\n",
      "  내용: Transformer (트랜스포머)\n",
      "\n",
      "정의: 자연어 처리에서 사용되는 신경망 아키텍처로, 병렬 연산과 장기 의존성 처리가 강점.\n",
      "예시: GPT, BERT 등의 모델이 트랜스포머 기...\n",
      "\n",
      "Chunk 3:\n",
      "  토큰 수: 621개\n",
      "  문자 수: 794자\n",
      "  내용: Reinforcement Learning (강화 학습)\n",
      "\n",
      "정의: 보상을 극대화하는 방향으로 행동을 학습하는 AI 기법.\n",
      "예시: 알파고가 바둑에서 최적의 수를 찾기 위해 강화 학습을...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. RecursiveCharacterTextSplitter Splitter와 비교\n",
    "print(\"\\n2. RecursiveCharacterTextSplitter (문자 기반 분할):\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "char_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,  # 문자 개수로 크기 지정 (토큰 200개 ≈ 문자 800개)\n",
    "    chunk_overlap=80,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_text(ai_terminology_text)\n",
    "print(f\"총 {len(char_chunks)}개 청크 생성\")\n",
    "\n",
    "for i, chunk in enumerate(char_chunks[:3], 1):\n",
    "    chunk_tokens = encoding.encode(chunk)\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  토큰 수: {len(chunk_tokens)}개\")\n",
    "    print(f\"  문자 수: {len(chunk)}자\")\n",
    "    print(f\"  내용: {chunk[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac8b9618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "토큰 기반 분할의 장점\n",
      "============================================================\n",
      "\n",
      "LLM 토큰 제한: 200개 토큰\n",
      "\n",
      "토큰 기반 분할 결과:\n",
      "  Chunk 1: 200개 토큰 OK\n",
      "  Chunk 2: 200개 토큰 OK\n",
      "  Chunk 3: 200개 토큰 OK\n",
      "  Chunk 4: 200개 토큰 OK\n",
      "  Chunk 5: 200개 토큰 OK\n",
      "  Chunk 6: 200개 토큰 OK\n",
      "  Chunk 7: 200개 토큰 OK\n",
      "  Chunk 8: 200개 토큰 OK\n",
      "  Chunk 9: 200개 토큰 OK\n",
      "  Chunk 10: 200개 토큰 OK\n",
      "  Chunk 11: 200개 토큰 OK\n",
      "  Chunk 12: 200개 토큰 OK\n",
      "  Chunk 13: 200개 토큰 OK\n",
      "  Chunk 14: 67개 토큰 OK\n",
      "\n",
      "토큰 제한 초과 청크: 0개\n",
      "\n",
      "문자 기반 분할 결과:\n",
      "  Chunk 1: 556개 토큰 초과\n",
      "  Chunk 2: 608개 토큰 초과\n",
      "  Chunk 3: 621개 토큰 초과\n",
      "  Chunk 4: 522개 토큰 초과\n",
      "  Chunk 5: 140개 토큰 OK\n",
      "\n",
      "토큰 제한 초과 청크: 4개\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 토큰 기반 분할의 장점 확인\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"토큰 기반 분할의 장점\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# LLM 토큰 제한 시뮬레이션\n",
    "max_tokens = 200  # 가상의 토큰 제한\n",
    "\n",
    "print(f\"\\nLLM 토큰 제한: {max_tokens}개 토큰\")\n",
    "print(\"\\n토큰 기반 분할 결과:\")\n",
    "over_limit_count = 0\n",
    "for i, chunk in enumerate(token_chunks, 1):\n",
    "    chunk_tokens = len(encoding.encode(chunk))\n",
    "    status = \"OK\" if chunk_tokens <= max_tokens else \"초과\"\n",
    "    if chunk_tokens > max_tokens:\n",
    "        over_limit_count += 1\n",
    "    print(f\"  Chunk {i}: {chunk_tokens}개 토큰 {status}\")\n",
    "\n",
    "print(f\"\\n토큰 제한 초과 청크: {over_limit_count}개\")\n",
    "\n",
    "print(\"\\n문자 기반 분할 결과:\")\n",
    "over_limit_count = 0\n",
    "for i, chunk in enumerate(char_chunks, 1):\n",
    "    chunk_tokens = len(encoding.encode(chunk))\n",
    "    status = \"OK\" if chunk_tokens <= max_tokens else \"초과\"\n",
    "    if chunk_tokens > max_tokens:\n",
    "        over_limit_count += 1\n",
    "    print(f\"  Chunk {i}: {chunk_tokens}개 토큰 {status}\")\n",
    "\n",
    "print(f\"\\n토큰 제한 초과 청크: {over_limit_count}개\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 다양한 encoding 테스트\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"다양한 인코딩 방식 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "encodings = [\n",
    "    (\"cl100k_base\", \"GPT-4, GPT-3.5-turbo\"),\n",
    "    (\"p50k_base\", \"GPT-3 (davinci)\"),\n",
    "    (\"r50k_base\", \"GPT-3 (ada, babbage, curie)\")\n",
    "]\n",
    "\n",
    "test_text = \"안녕하세요! AI와 머신러닝에 대해 배워보겠습니다. Hello, let's learn about AI and Machine Learning!\"\n",
    "\n",
    "for encoding_name, description in encodings:\n",
    "    enc = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = enc.encode(test_text)\n",
    "    print(f\"\\n{encoding_name} ({description}):\")\n",
    "    print(f\"  토큰 수: {len(tokens)}개\")\n",
    "    print(f\"  토큰 예시: {tokens[:10]}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 활용 예제\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"실무 활용 예제\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 다양한 chunk_size로 테스트\n",
    "chunk_sizes = [100, 200, 500]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"\\nchunk_size={chunk_size} 토큰:\")\n",
    "    splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size//10,  # 10% 중복\n",
    "        encoding_name=\"cl100k_base\"\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_text(ai_terminology_text)\n",
    "    avg_tokens = sum(len(encoding.encode(chunk)) for chunk in chunks) / len(chunks)\n",
    "    \n",
    "    print(f\"  총 청크 수: {len(chunks)}개\")\n",
    "    print(f\"  평균 토큰 수: {avg_tokens:.1f}개\")\n",
    "    print(f\"  첫 번째 청크 토큰 수: {len(encoding.encode(chunks[0]))}개\")\n",
    "\n",
    "# ==========================================\n",
    "# 메타데이터 포함 문서 분할\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"메타데이터 포함 Document 분할\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Document 객체 생성\n",
    "document = Document(\n",
    "    page_content=ai_terminology_text,\n",
    "    metadata={\"source\": \"ai-terminology.txt\", \"type\": \"glossary\"}\n",
    ")\n",
    "\n",
    "# Document 분할\n",
    "doc_chunks = token_splitter.split_documents([document])\n",
    "\n",
    "print(f\"Document 분할 결과: {len(doc_chunks)}개 청크\")\n",
    "for i, doc_chunk in enumerate(doc_chunks[:2], 1):\n",
    "    tokens_count = len(encoding.encode(doc_chunk.page_content))\n",
    "    print(f\"\\nDocument Chunk {i}:\")\n",
    "    print(f\"  토큰 수: {tokens_count}개\")\n",
    "    print(f\"  메타데이터: {doc_chunk.metadata}\")\n",
    "    print(f\"  내용: {doc_chunk.page_content[:100]}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 실무 가이드\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TokenTextSplitter 실무 가이드\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "guide = \"\"\"\n",
    "TokenTextSplitter 사용 가이드:\n",
    "\n",
    "1. 언제 사용?\n",
    "   - OpenAI API 사용 시 (GPT-3.5, GPT-4)\n",
    "   - 정확한 토큰 수 제어가 필요할 때\n",
    "   - LLM 비용 최적화가 중요할 때\n",
    "\n",
    "2. 권장 설정:\n",
    "   - GPT-3.5-turbo: chunk_size=3000, overlap=300\n",
    "   - GPT-4: chunk_size=7000, overlap=700\n",
    "   - 임베딩용: chunk_size=500, overlap=50\n",
    "\n",
    "3. 인코딩 선택:\n",
    "   - cl100k_base: GPT-4, GPT-3.5-turbo (권장)\n",
    "   - p50k_base: GPT-3 (legacy)\n",
    "\n",
    "4. 장점:\n",
    "   - 정확한 토큰 수 제어\n",
    "   - LLM 토큰 제한 준수\n",
    "   - 비용 예측 가능\n",
    "   - API 호출 안정성\n",
    "\n",
    "5. 단점:\n",
    "   - 의미 단위 분할 어려움\n",
    "   - 문장 중간에서 분할 가능\n",
    "   - 토큰화 오버헤드\n",
    "\n",
    "6. 최적화 팁:\n",
    "   - chunk_overlap을 chunk_size의 10-15%로 설정\n",
    "   - 중요한 문서는 RecursiveCharacterTextSplitter와 병행 사용\n",
    "   - 토큰 수와 의미 보존의 균형점 찾기\n",
    "\"\"\"\n",
    "\n",
    "print(guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2248589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6009baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# GPT-2 모델의 토크나이저 로드\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 데이터 파일 읽기\n",
    "file_path = \"../data/ai-terminology.txt\"\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "print(\" 원본 텍스트 미리보기:\\n\", file_content[:200])\n",
    "\n",
    "# CharacterTextSplitter 설정 (Hugging Face 토크나이저 사용)\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,  # 각 청크 크기 (토큰 기준 아님)\n",
    "    chunk_overlap=50,  # 청크 간 중복 부분\n",
    ")\n",
    "\n",
    "# 텍스트 분할 수행\n",
    "split_texts = text_splitter.split_text(file_content)\n",
    "\n",
    "# 분할된 텍스트 출력\n",
    "print(f\"\\n 총 {len(split_texts)}개의 청크로 분할됨\\n\")\n",
    "for i, chunk in enumerate(split_texts[:5]):  # 처음 5개만 출력\n",
    "    print(f\" Chunk {i+1} ({len(chunk)}자):\\n{chunk}\\n\")\n",
    "\n",
    "# 토크나이저로 텍스트를 토큰 단위로 변환하여 확인\n",
    "tokenized_example = hf_tokenizer.tokenize(split_texts[0])\n",
    "print(f\"\\n 첫 번째 청크의 토큰 개수: {len(tokenized_example)}\")\n",
    "print(\" 첫 번째 청크의 토큰 리스트:\", tokenized_example[:20])  # 앞 20개만 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd61dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# GPT-2 모델의 토크나이저 로드\n",
    "print(\"GPT-2 토크나이저 로딩 중...\")\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "print(\"토크나이저 로딩 완료\")\n",
    "\n",
    "# 데이터 파일 읽기\n",
    "file_path = \"../data/ai-terminology.txt\"\n",
    "\n",
    "# 파일 읽기\n",
    "try:\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        file_content = f.read()\n",
    "    print(f\"파일 읽기 성공: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"파일 읽기 실패: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"원본 텍스트 미리보기:\")\n",
    "print(\"-\" * 50)\n",
    "print(file_content[:200] + \"...\")\n",
    "print(f\"\\n전체 텍스트 길이: {len(file_content)}자\")\n",
    "\n",
    "# ==========================================\n",
    "# GPT-2 토크나이저 기본 정보\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPT-2 토크나이저 기본 정보\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"모델명: {hf_tokenizer.name_or_path}\")\n",
    "print(f\"어휘 크기: {hf_tokenizer.vocab_size:,}개\")\n",
    "print(f\"최대 입력 길이: {hf_tokenizer.model_max_length:,} 토큰\")\n",
    "\n",
    "# 특수 토큰 확인\n",
    "print(f\"패딩 토큰: {hf_tokenizer.pad_token}\")\n",
    "print(f\"시작 토큰: {hf_tokenizer.bos_token}\")\n",
    "print(f\"끝 토큰: {hf_tokenizer.eos_token}\")\n",
    "print(f\"언노운 토큰: {hf_tokenizer.unk_token}\")\n",
    "\n",
    "# 전체 텍스트의 토큰 개수 확인\n",
    "total_tokens = len(hf_tokenizer.tokenize(file_content))\n",
    "print(f\"\\n원본 텍스트 토큰 개수: {total_tokens:,}개\")\n",
    "print(f\"문자 대 토큰 비율: {len(file_content)/total_tokens:.2f} (문자/토큰)\")\n",
    "\n",
    "# ==========================================\n",
    "# 토크나이저 동작 확인\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"토크나이저 동작 확인\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 예제 텍스트로 토크나이저 테스트\n",
    "test_text = \"RAG는 검색 기반의 텍스트 생성 모델입니다. Hello, AI!\"\n",
    "print(f\"테스트 텍스트: {test_text}\")\n",
    "\n",
    "# 1. 토큰화 (tokenize)\n",
    "tokens = hf_tokenizer.tokenize(test_text)\n",
    "print(f\"\\n1. 토큰화 결과: {tokens}\")\n",
    "print(f\"   토큰 개수: {len(tokens)}개\")\n",
    "\n",
    "# 2. 인코딩 (encode) - 토큰을 ID로 변환\n",
    "token_ids = hf_tokenizer.encode(test_text)\n",
    "print(f\"\\n2. 인코딩 결과: {token_ids}\")\n",
    "print(f\"   토큰 ID 개수: {len(token_ids)}개\")\n",
    "\n",
    "# 3. 디코딩 (decode) - ID를 다시 텍스트로 변환\n",
    "decoded_text = hf_tokenizer.decode(token_ids)\n",
    "print(f\"\\n3. 디코딩 결과: {decoded_text}\")\n",
    "\n",
    "# 4. 개별 토큰 ID 확인\n",
    "print(f\"\\n4. 토큰별 ID 매핑:\")\n",
    "for i, (token, token_id) in enumerate(zip(tokens, token_ids), 1):\n",
    "    print(f\"   {i:2d}. '{token}' -> {token_id}\")\n",
    "\n",
    "# ==========================================\n",
    "# CharacterTextSplitter with HuggingFace 토크나이저\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CharacterTextSplitter with HuggingFace 토크나이저\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# CharacterTextSplitter 설정 (Hugging Face 토크나이저 사용)\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,     # 토큰 개수 기준\n",
    "    chunk_overlap=50,   # 토큰 단위 중복\n",
    "    separator=\"\\n\\n\"    # 구분자 설정\n",
    ")\n",
    "\n",
    "# 텍스트 분할 수행\n",
    "split_texts = text_splitter.split_text(file_content)\n",
    "\n",
    "print(f\"총 {len(split_texts)}개의 청크로 분할됨\")\n",
    "print(\"\\n처음 3개 청크 분석:\")\n",
    "\n",
    "for i, chunk in enumerate(split_texts[:3], 1):\n",
    "    chunk_tokens = hf_tokenizer.tokenize(chunk)\n",
    "    chunk_token_ids = hf_tokenizer.encode(chunk)\n",
    "    \n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  문자 수: {len(chunk):,}자\")\n",
    "    print(f\"  토큰 수: {len(chunk_tokens):,}개\")\n",
    "    print(f\"  토큰 ID 수: {len(chunk_token_ids):,}개\")\n",
    "    print(f\"  내용 미리보기: {chunk[:100]}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 다양한 chunk_size 비교\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"다양한 chunk_size 설정 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "chunk_sizes = [100, 300, 500]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"\\nchunk_size={chunk_size} 토큰:\")\n",
    "    \n",
    "    splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        hf_tokenizer,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size//10,  # 10% 중복\n",
    "        separator=\"\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_text(file_content)\n",
    "    \n",
    "    # 통계 계산\n",
    "    total_tokens = sum(len(hf_tokenizer.tokenize(chunk)) for chunk in chunks)\n",
    "    avg_tokens = total_tokens / len(chunks) if chunks else 0\n",
    "    \n",
    "    print(f\"  총 청크 수: {len(chunks)}개\")\n",
    "    print(f\"  평균 토큰 수: {avg_tokens:.1f}개\")\n",
    "    print(f\"  전체 토큰 수: {total_tokens:,}개\")\n",
    "    \n",
    "    # 토큰 수 분포 확인\n",
    "    token_counts = [len(hf_tokenizer.tokenize(chunk)) for chunk in chunks]\n",
    "    min_tokens = min(token_counts) if token_counts else 0\n",
    "    max_tokens = max(token_counts) if token_counts else 0\n",
    "    \n",
    "    print(f\"  토큰 수 범위: {min_tokens}~{max_tokens}개\")\n",
    "\n",
    "# ==========================================\n",
    "# 토크나이저별 비교 (GPT-2 vs tiktoken)\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"토크나이저별 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# tiktoken과 비교 (OpenAI 토크나이저)\n",
    "try:\n",
    "    import tiktoken\n",
    "    \n",
    "    # OpenAI 토크나이저\n",
    "    openai_encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # 같은 텍스트로 비교\n",
    "    comparison_text = \"RAG는 검색 기반의 텍스트 생성 모델입니다. AI와 머신러닝을 활용합니다.\"\n",
    "    \n",
    "    # GPT-2 토큰화\n",
    "    gpt2_tokens = hf_tokenizer.tokenize(comparison_text)\n",
    "    gpt2_token_count = len(gpt2_tokens)\n",
    "    \n",
    "    # OpenAI 토큰화\n",
    "    openai_tokens = openai_encoding.encode(comparison_text)\n",
    "    openai_token_count = len(openai_tokens)\n",
    "    \n",
    "    print(f\"비교 텍스트: {comparison_text}\")\n",
    "    print(f\"\\nGPT-2 토크나이저:\")\n",
    "    print(f\"  토큰 수: {gpt2_token_count}개\")\n",
    "    print(f\"  토큰: {gpt2_tokens}\")\n",
    "    \n",
    "    print(f\"\\nOpenAI 토크나이저 (cl100k_base):\")\n",
    "    print(f\"  토큰 수: {openai_token_count}개\")\n",
    "    print(f\"  토큰 ID: {openai_tokens}\")\n",
    "    \n",
    "    print(f\"\\n토큰 수 차이: {abs(gpt2_token_count - openai_token_count)}개\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"tiktoken이 설치되지 않음. GPT-2 토크나이저만 사용합니다.\")\n",
    "\n",
    "# ==========================================\n",
    "# 한국어 처리 성능 테스트\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"한국어 처리 성능 테스트\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "korean_texts = [\n",
    "    \"안녕하세요\",\n",
    "    \"인공지능과 머신러닝\",\n",
    "    \"자연어 처리 기술\",\n",
    "    \"딥러닝 모델 학습\",\n",
    "    \"검색 기반 생성 모델\"\n",
    "]\n",
    "\n",
    "print(\"한국어 텍스트별 토큰화 결과:\")\n",
    "for i, text in enumerate(korean_texts, 1):\n",
    "    tokens = hf_tokenizer.tokenize(text)\n",
    "    token_ids = hf_tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"\\n{i}. '{text}'\")\n",
    "    print(f\"   토큰: {tokens}\")\n",
    "    print(f\"   토큰 수: {len(tokens)}개\")\n",
    "    print(f\"   문자당 토큰: {len(tokens)/len(text):.2f}\")\n",
    "\n",
    "# ==========================================\n",
    "# Document 객체와 함께 사용\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Document 객체와 함께 사용\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Document 객체 생성\n",
    "document = Document(\n",
    "    page_content=file_content,\n",
    "    metadata={\n",
    "        \"source\": \"ai-terminology.txt\",\n",
    "        \"type\": \"glossary\",\n",
    "        \"tokenizer\": \"gpt2\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Document 분할\n",
    "doc_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "doc_chunks = doc_splitter.split_documents([document])\n",
    "\n",
    "print(f\"Document 분할 결과: {len(doc_chunks)}개 청크\")\n",
    "\n",
    "for i, doc_chunk in enumerate(doc_chunks[:2], 1):\n",
    "    token_count = len(hf_tokenizer.tokenize(doc_chunk.page_content))\n",
    "    print(f\"\\nDocument Chunk {i}:\")\n",
    "    print(f\"  토큰 수: {token_count}개\")\n",
    "    print(f\"  메타데이터: {doc_chunk.metadata}\")\n",
    "    print(f\"  내용: {doc_chunk.page_content[:100]}...\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
